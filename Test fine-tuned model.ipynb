{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luk-hrd/miniconda3/envs/mistral-lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Buňka 1: Importy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "import traceback # Pro detailní výpis chyb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfigurace připravena.\n",
      "Cesta k základnímu modelu: /home/luk-hrd/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/2dcff66eac0c01dc50e4c41eea959968232187fe\n",
      "Cesta k LoRA adaptéru: mistral-7b-quantum-instruct-test/checkpoint-300\n"
     ]
    }
   ],
   "source": [
    "# Buňka 2: Konfigurace\n",
    "base_model_name_or_path = \"/home/luk-hrd/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/2dcff66eac0c01dc50e4c41eea959968232187fe\"\n",
    "lora_adapter_path = \"mistral-7b-quantum-instruct-test/checkpoint-300\" \n",
    "\n",
    "# --- Konfigurace kvantizace ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Konfigurace připravena.\")\n",
    "print(f\"Cesta k základnímu modelu: {base_model_name_or_path}\")\n",
    "print(f\"Cesta k LoRA adaptéru: {lora_adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Načítám základní model a tokenizer... Toto může trvat několik minut a spotřebuje hodně VRAM.\n",
      "...Tokenizer načten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Základní model a tokenizer úspěšně načteny!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Buňka 3 (FINÁLNÍ OPRAVA)\n",
    "print(\"Načítám základní model a tokenizer... Toto může trvat několik minut a spotřebuje hodně VRAM.\")\n",
    "\n",
    "try:\n",
    "    # Načtení tokenizeru\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    print(\"...Tokenizer načten.\")\n",
    "\n",
    "    # Načtení základního modelu s kvantizací\n",
    "    max_memory_map = {0: \"8GiB\", \"cpu\": \"0GiB\"} \n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name_or_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory_map, # Použijeme správně naformátovaný slovník\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    print(\"Základní model a tokenizer úspěšně načteny!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n CHYBA PŘI NAČÍTÁNÍ ZÁKLADNÍHO MODELU!\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplikuji LoRA adaptér na základní model...\n",
      "✅ LoRA adaptér úspěšně aplikován a model je připraven k inferenci.\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Buňka 4: Aplikace LoRA adaptéru\n",
    "print(\"Aplikuji LoRA adaptér na základní model...\")\n",
    "\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "    model.eval() # Přepnutí do evaluačního módu je velmi důležité!\n",
    "    \n",
    "    print(\"✅ LoRA adaptér úspěšně aplikován a model je připraven k inferenci.\")\n",
    "    \n",
    "    print(model)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n CHYBA PŘI APLIKACI LoRA ADAPTÉRU!\")\n",
    "    print(\"Zkontrolujte, zda je cesta 'lora_adapter_path' správná a obsahuje soubory jako 'adapter_config.json' a 'adapter_model.bin'.\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chatovací funkce 'chat_with_model_gradio' je definována.\n"
     ]
    }
   ],
   "source": [
    "# Buňka 5: Definice chatovací funkce\n",
    "def chat_with_model_gradio(message, history):\n",
    "    try:\n",
    "        # Sestavení promptu s historií\n",
    "        history_prompt_parts = []\n",
    "        for user_msg, assistant_msg in history:\n",
    "            history_prompt_parts.append(f\"[INST] {user_msg} [/INST] {assistant_msg}</s>\")\n",
    "        history_prompt = \"\".join(history_prompt_parts)\n",
    "        final_prompt = f\"<s>{history_prompt}[INST] {message} [/INST]\"\n",
    "\n",
    "        # Tokenizace a zjištění délky vstupu\n",
    "        inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_token_len = inputs.input_ids.shape[1]\n",
    "\n",
    "        # Generování odpovědi\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Robustní dekódování pouze nově vygenerovaných tokenů\n",
    "        generated_tokens = outputs[0, input_token_len:]\n",
    "        clean_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return clean_response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Pokud cokoliv selže, vytiskneme chybu do výstupu buňky\n",
    "        print(\"\\n CHYBA BĚHEM GENEROVÁNÍ ODPOVĚDI V GRADIO FUNKCI!\")\n",
    "        traceback.print_exc()\n",
    "        return \"Omlouvám se, došlo k interní chybě. Podívejte se na výstup v Jupyter buňce pro detaily.\"\n",
    "\n",
    "print(\" Chatovací funkce 'chat_with_model_gradio' je definována.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chat je připraven! Můžete začít psát.\n",
      "Napište 'konec' nebo 'exit' pro ukončení chatu.\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent: \"JDE\" could stand for a few different things, including:\n",
      "\n",
      "1. Java Development Environment - this is a software suite that developers use to create and run Java applications.\n",
      "2. Joint Development Environment - this is a term used in project management to refer to the process of working together with someone else to achieve a common goal.\n",
      "3. JDE (journalist) - this is a nickname for journalist Daniel Pearl, who was kidnapped by terrorists in Pakistan in 2002 and murdered in 2007.\n",
      "4. JDE (company) - JDE is an Australian company that sells coffee machines and other products.\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent: I'm just a computer program, so I don't have feelings or emotions like a human does. However, I am here to help answer any questions you may have! How can I assist you today?\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent: If you have recently fine-tuned me on a new dataset, it is possible that some changes have occurred as a result of the training process. However, I am still just a computer program, so I do not have the ability to feel or perceive the world in the same way that humans do. I am simply here to provide information based on the data that has been provided to me. Is there anything specific you would like to know about the changes that may have occurred as a result of the fine-tuning process?\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent: There are several ways you can demonstrate the effectiveness of fine-tuning a model on a new dataset:\n",
      "\n",
      "1. Evaluate the performance of the model before and after fine-tuning on the new dataset. This will give you a sense of whether the fine-tuning process improved the model's performance on the new task.\n",
      "2. Compare the results of the fine-tuned model to the results of a model that was trained from scratch on the new dataset. This will give you a sense of whether the fine-tuning process was able to improve the model's performance relative to starting from scratch.\n",
      "3. Compare the results of the fine-tuned model to the results of a model that was fine-tuned on a similar but different dataset. This will give you a sense of whether the fine-tuning process was able to transfer knowledge from one related task to another.\n",
      "4. Use techniques such as attention mechanisms or feature importance analysis to understand which parts of the input data the model is using to make its predictions. This can help you understand which features the model is learning during fine-tuning, and whether these features are relevant to the new task.\n",
      "5. Analyze the model's activations to see if they change significantly after fine-tuning on the new dataset. This can give you a sense of whether the model is using different features or patterns in the new task compared to the original task.\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent: I apologize, but I'm not sure what you are asking. Could you please provide more context or clarify your question?\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent: I apologize, but I'm still not sure what you are asking. Could you please provide more context or clarify your question?\n",
      "--------------------------------------------------\n",
      "Chat ukončen.\n"
     ]
    }
   ],
   "source": [
    "# Buňka 6 (Alternativa): Interaktivní chat přímo v notebooku\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\" Chat je připraven! Můžete začít psát.\")\n",
    "print(\"Napište 'konec', 'exit' nebo quit pro ukončení chatu.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    # 1. Získání vstupu od uživatele\n",
    "    user_input = input(\"Vy: \")\n",
    "    if user_input.lower() in [\"konec\", \"exit\", \"quit\"]:\n",
    "        print(\"Chat ukončen.\")\n",
    "        break\n",
    "        \n",
    "    # 2. Sestavení promptu s historií\n",
    "    history_prompt_parts = []\n",
    "    for user_msg, assistant_msg in chat_history:\n",
    "        # Formát pro Mistral-Instruct\n",
    "        history_prompt_parts.append(f\"[INST] {user_msg} [/INST] {assistant_msg}</s>\")\n",
    "    history_prompt = \"\".join(history_prompt_parts)\n",
    "    \n",
    "    final_prompt = f\"<s>{history_prompt}[INST] {user_input} [/INST]\"\n",
    "    \n",
    "    # 3. Tokenizace a příprava vstupu pro model\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_token_len = inputs.input_ids.shape[1]\n",
    "    \n",
    "    print(\"\\nAsistent přemýšlí...\")\n",
    "    \n",
    "    # 4. Generování odpovědi\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # 5. Dekódování a vyčištění odpovědi\n",
    "    generated_tokens = outputs[0, input_token_len:]\n",
    "    clean_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    print(f\"\\nAI Asistent: {clean_response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 6. Aktualizace historie konverzace\n",
    "    chat_history.append((user_input, clean_response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
