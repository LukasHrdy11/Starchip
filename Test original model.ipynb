{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luk-hrd/miniconda3/envs/mistral-lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knihovny úspěšně naimportovány.\n"
     ]
    }
   ],
   "source": [
    "# Buňka 1: Importy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Konfigurace připravena.\n",
      "Bude se načítat originální model: mistralai/Mistral-7B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "# Buňka 2: Konfigurace\n",
    "# Použijeme přímo identifikátor modelu z Hugging Face Hub\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# --- Konfigurace kvantizace (zůstává stejná, je nutná pro běh na vaší GPU) ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Bude se načítat originální model: {base_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer načten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00,  9.13it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Originální model je načten a připraven k chatu!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    print(\"Tokenizer načten.\")\n",
    "\n",
    "    # Explicitně řekneme, aby se pro GPU s ID 0 použilo maximum paměti\n",
    "    # a pro CPU se nepoužívala žádná paměť na offloadování vrstev.\n",
    "    max_memory_map = {0: \"8GiB\", \"cpu\": \"0GiB\"} \n",
    "\n",
    "    # Načtení kvantizovaného modelu s explicitním řízením paměti\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory_map, # Přidán osvědčený parametr\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Důležité: Přepneme model do evaluačního módu\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"\\n Originální model je načten a připraven k chatu!\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"\\n CHYBA PŘI NAČÍTÁNÍ MODELU!\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat s ORIGINÁLNÍM modelem je připraven! Můžete začít psát.\n",
      "Napište 'konec' nebo 'exit' pro ukončení chatu.\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent (Originál): A quantum state is a mathematical description of the physical properties of a quantum system at a particular time. It includes information about the probability distribution of different possible outcomes when measurements are made on the system. In other words, it represents the current state of the system in terms of its potential future behavior. Quantum states can be represented using mathematical equations such as Schrödinger's equation and can be visualized using diagrams like density matrices or wave functions.\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent (Originál): A quantum state is a mathematical description of the physical properties of a quantum system at a particular time. It includes information about the probability distribution of different possible outcomes when measurements are made on the system. In other words, it represents the current state of the system in terms of its potential future behavior. Quantum states can be represented using mathematical equations such as Schrödinger's equation and can be visualized using diagrams like density matrices or wave functions.\n",
      "--------------------------------------------------\n",
      "Chat ukončen.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "print(\"Chat s ORIGINÁLNÍM modelem je připraven! Můžete začít psát.\")\n",
    "print(\"Napište 'konec' nebo 'exit' pro ukončení chatu.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Vy: \")\n",
    "    if user_input.lower() in [\"konec\", \"exit\", \"quit\"]:\n",
    "        print(\"Chat ukončen.\")\n",
    "        break\n",
    "        \n",
    "    # 2. Sestavení promptu s historií (formát pro Mistral-Instruct)\n",
    "    history_prompt_parts = []\n",
    "    for user_msg, assistant_msg in chat_history:\n",
    "        history_prompt_parts.append(f\"[INST] {user_msg} [/INST] {assistant_msg}</s>\")\n",
    "    history_prompt = \"\".join(history_prompt_parts)\n",
    "    \n",
    "    final_prompt = f\"<s>{history_prompt}[INST] {user_input} [/INST]\"\n",
    "    \n",
    "    # 3. Tokenizace a příprava vstupu\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_token_len = inputs.input_ids.shape[1]\n",
    "    \n",
    "    print(\"\\nAsistent přemýšlí...\")\n",
    "    \n",
    "    # 4. Generování odpovědi\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # 5. Dekódování a vyčištění odpovědi\n",
    "    generated_tokens = outputs[0, input_token_len:]\n",
    "    clean_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    print(f\"\\nAI Asistent (Originál): {clean_response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 6. Aktualizace historie konverzace\n",
    "    chat_history.append((user_input, clean_response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
