{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luk-hrd/miniconda3/envs/mistral-lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knihovny úspěšně naimportovány.\n"
     ]
    }
   ],
   "source": [
    "# Buňka 1: Importy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"✅ Knihovny úspěšně naimportovány.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Konfigurace připravena.\n",
      "Bude se načítat originální model: mistralai/Mistral-7B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "# Buňka 2: Konfigurace\n",
    "# Použijeme přímo identifikátor modelu z Hugging Face Hub\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# --- Konfigurace kvantizace (zůstává stejná, je nutná pro běh na vaší GPU) ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"✅ Konfigurace připravena.\")\n",
    "print(f\"Bude se načítat originální model: {base_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Načítám originální model a tokenizer... Toto může trvat několik minut.\n",
      "...Tokenizer načten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Originální model je načten a připraven k chatu!\n"
     ]
    }
   ],
   "source": [
    "# Buňka 3 (OPRAVENÁ VERZE PRO ORIGINÁLNÍ MODEL)\n",
    "print(\"⏳ Načítám originální model a tokenizer... Toto může trvat několik minut.\")\n",
    "\n",
    "try:\n",
    "    # Načtení tokenizeru\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    print(\"...Tokenizer načten.\")\n",
    "\n",
    "    # ------ ZDE JE OPĚT KLÍČOVÁ OPRAVA ------\n",
    "    # Explicitně řekneme, aby se pro GPU s ID 0 použilo maximum paměti\n",
    "    # a pro CPU se nepoužívala žádná paměť na offloadování vrstev.\n",
    "    # Použijeme stejný trik, který fungoval u fine-tuned modelu.\n",
    "    max_memory_map = {0: \"8GiB\", \"cpu\": \"0GiB\"} \n",
    "\n",
    "    # Načtení kvantizovaného modelu s explicitním řízením paměti\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory_map, # Přidán osvědčený parametr\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Důležité: Přepneme model do evaluačního módu\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"\\n✅ Originální model je načten a připraven k chatu!\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"\\n❌ CHYBA PŘI NAČÍTÁNÍ MODELU!\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chat s ORIGINÁLNÍM modelem je připraven! Můžete začít psát.\n",
      "Napište 'konec' nebo 'exit' pro ukončení chatu.\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent (Originál): In quantum mechanics, a quantum state is a mathematical representation of the current state or condition of a quantum system. It encodes all the information about the system that can be known through measurements, including its energy, momentum, spin, and other observable properties.\n",
      "\n",
      "Quantum states are typically represented using mathematical equations called wave functions, which describe the probability distribution of particles in space and time. The wave function contains all the information about the system's behavior that can be observed experimentally, including its energy, momentum, and spin.\n",
      "\n",
      "The concept of quantum states is fundamental to quantum mechanics, as it allows us to understand the behavior of matter and energy at the atomic and subatomic level. Quantum states play a crucial role in many applications of quantum mechanics, including quantum computing, cryptography, and sensing.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# 1. Získání vstupu od uživatele\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVy: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkonec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat ukončen.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mistral-lora/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mistral-lora/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Buňka 4: Interaktivní chat s originálním modelem\n",
    "\n",
    "# Seznam pro uchování historie konverzace\n",
    "chat_history = []\n",
    "\n",
    "print(\"✅ Chat s ORIGINÁLNÍM modelem je připraven! Můžete začít psát.\")\n",
    "print(\"Napište 'konec' nebo 'exit' pro ukončení chatu.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    # 1. Získání vstupu od uživatele\n",
    "    user_input = input(\"Vy: \")\n",
    "    if user_input.lower() in [\"konec\", \"exit\", \"quit\"]:\n",
    "        print(\"Chat ukončen.\")\n",
    "        break\n",
    "        \n",
    "    # 2. Sestavení promptu s historií (formát pro Mistral-Instruct)\n",
    "    history_prompt_parts = []\n",
    "    for user_msg, assistant_msg in chat_history:\n",
    "        history_prompt_parts.append(f\"[INST] {user_msg} [/INST] {assistant_msg}</s>\")\n",
    "    history_prompt = \"\".join(history_prompt_parts)\n",
    "    \n",
    "    final_prompt = f\"<s>{history_prompt}[INST] {user_input} [/INST]\"\n",
    "    \n",
    "    # 3. Tokenizace a příprava vstupu\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_token_len = inputs.input_ids.shape[1]\n",
    "    \n",
    "    print(\"\\nAsistent přemýšlí...\")\n",
    "    \n",
    "    # 4. Generování odpovědi\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # 5. Dekódování a vyčištění odpovědi\n",
    "    generated_tokens = outputs[0, input_token_len:]\n",
    "    clean_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    print(f\"\\nAI Asistent (Originál): {clean_response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 6. Aktualizace historie konverzace\n",
    "    chat_history.append((user_input, clean_response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
