{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Načítám kvantizovaný model a tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vygenerovaná device_map:\n",
      "OrderedDict([('', 'cpu')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:48<00:00, 54.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplikuji LoRA adaptér...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luk-hrd/miniconda3/envs/mistral-lora/lib/python3.10/site-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model je připraven!\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "\n",
    "# --- 1. Definujte cesty a konfigurace ---\n",
    "base_model_name_or_path = \"/home/luk-hrd/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/2dcff66eac0c01dc50e4c41eea959968232187fe\"\n",
    "lora_adapter_path = \"mistral-7b-quantum-instruct-v1/final_adapter\" \n",
    "\n",
    "# --- 2. Upravená konfigurace kvantizace ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# --- 3. Načtení modelu s vlastní device_map ---\n",
    "print(\"Načítám kvantizovaný model a tokenizer...\")\n",
    "\n",
    "# Vytvoříme si vlastní device_map místo \"auto\"\n",
    "with init_empty_weights():\n",
    "    model_empty = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name_or_path, \n",
    "        config=AutoConfig.from_pretrained(base_model_name_or_path)\n",
    "    )\n",
    "\n",
    "# !!! ZDE JE FINÁLNÍ OPRAVA: Odstraněn argument 'quantization_config' !!!\n",
    "device_map = infer_auto_device_map(\n",
    "    model_empty,\n",
    "    dtype=torch.bfloat16,\n",
    "    no_split_module_classes=[\"MistralDecoderLayer\"]\n",
    ")\n",
    "print(\"Vygenerovaná device_map:\")\n",
    "print(device_map)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name_or_path,\n",
    "    quantization_config=bnb_config, # Tento argument zde zůstává, je to správně\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Zbytek kódu je stejný\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Aplikuji LoRA adaptér...\")\n",
    "model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model je připraven!\")\n",
    "\n",
    "\n",
    "# --- Funkce pro Gradio a spuštění UI (beze změny) ---\n",
    "def chat_with_model_gradio(message, history):\n",
    "    history_prompt = \"\"\n",
    "    for user_msg, assistant_msg in history:\n",
    "        history_prompt += f\"[INST] {user_msg} [/INST] {assistant_msg}</s>\"\n",
    "\n",
    "    final_prompt = f\"<s>{history_prompt}[INST] {message} [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    clean_response = response.split(\"[/INST]\")[-1].strip()\n",
    "    \n",
    "    return clean_response\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    fn=chat_with_model_gradio,\n",
    "    title=\"PhD AI Assistant (Quantum Mechanics)\",\n",
    "    description=\"Chat with me about quantum mechanics and computing.\",\n",
    "    theme=\"soft\",\n",
    "     examples=[\n",
    "        [\"Explain the Bell state in detail.\"],\n",
    "        [\"Write a Qiskit code snippet to create a GHZ state.\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU je dostupná!\n",
      "Název zařízení: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Celková paměť: 8.32 GB\n",
      "Volná paměť: 0.57 GB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU je dostupná!\")\n",
    "    print(f\"Název zařízení: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Celková paměť: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Volná paměť: {torch.cuda.mem_get_info(0)[0] / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"POZOR: GPU není dostupná. PyTorch používá pouze CPU.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
