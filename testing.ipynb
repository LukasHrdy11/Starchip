{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luk-hrd/miniconda3/envs/mistral-lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knihovny úspěšně naimportovány.\n"
     ]
    }
   ],
   "source": [
    "# Buňka 1: Importy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "import traceback # Pro detailní výpis chyb\n",
    "\n",
    "print(\"✅ Knihovny úspěšně naimportovány.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Konfigurace připravena.\n",
      "Cesta k základnímu modelu: /home/luk-hrd/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/2dcff66eac0c01dc50e4c41eea959968232187fe\n",
      "Cesta k LoRA adaptéru: mistral-7b-quantum-instruct-v1/final_adapter\n"
     ]
    }
   ],
   "source": [
    "# Buňka 2: Konfigurace\n",
    "# --- Zkontrolujte, zda jsou tyto cesty 100% správné! ---\n",
    "base_model_name_or_path = \"/home/luk-hrd/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/2dcff66eac0c01dc50e4c41eea959968232187fe\"\n",
    "lora_adapter_path = \"mistral-7b-quantum-instruct-v1/final_adapter\" \n",
    "\n",
    "# --- Konfigurace kvantizace ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"✅ Konfigurace připravena.\")\n",
    "print(f\"Cesta k základnímu modelu: {base_model_name_or_path}\")\n",
    "print(f\"Cesta k LoRA adaptéru: {lora_adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Načítám základní model a tokenizer... Toto může trvat několik minut a spotřebuje hodně VRAM.\n",
      "...Tokenizer načten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Základní model a tokenizer úspěšně načteny!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Buňka 3 (FINÁLNÍ OPRAVA)\n",
    "print(\"⏳ Načítám základní model a tokenizer... Toto může trvat několik minut a spotřebuje hodně VRAM.\")\n",
    "\n",
    "try:\n",
    "    # Načtení tokenizeru\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    print(\"...Tokenizer načten.\")\n",
    "\n",
    "    # ------ ZDE JE FINÁLNÍ ZMĚNA ------\n",
    "    # Explicitně řekneme, aby se pro GPU s ID 0 použilo maximum paměti\n",
    "    # a pro CPU se nepoužívala žádná paměť na offloadování vrstev.\n",
    "    # To řeší oba předchozí problémy.\n",
    "    max_memory_map = {0: \"8GiB\", \"cpu\": \"0GiB\"} \n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name_or_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory_map, # Použijeme správně naformátovaný slovník\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    print(\"✅ Základní model a tokenizer úspěšně načteny!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n❌ CHYBA PŘI NAČÍTÁNÍ ZÁKLADNÍHO MODELU!\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Aplikuji LoRA adaptér na základní model...\n",
      "✅ LoRA adaptér úspěšně aplikován a model je připraven k inferenci.\n"
     ]
    }
   ],
   "source": [
    "# Buňka 4: Aplikace LoRA adaptéru\n",
    "print(\"⏳ Aplikuji LoRA adaptér na základní model...\")\n",
    "\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "    model.eval() # Přepnutí do evaluačního módu je velmi důležité!\n",
    "    \n",
    "    print(\"✅ LoRA adaptér úspěšně aplikován a model je připraven k inferenci.\")\n",
    "    \n",
    "    # Nepovinné: Vytiskneme si strukturu modelu, abychom viděli LoRA vrstvy\n",
    "    # print(model)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n❌ CHYBA PŘI APLIKACI LoRA ADAPTÉRU!\")\n",
    "    print(\"Zkontrolujte, zda je cesta 'lora_adapter_path' správná a obsahuje soubory jako 'adapter_config.json' a 'adapter_model.bin'.\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chatovací funkce 'chat_with_model_gradio' je definována.\n"
     ]
    }
   ],
   "source": [
    "# Buňka 5: Definice chatovací funkce\n",
    "def chat_with_model_gradio(message, history):\n",
    "    try:\n",
    "        # Sestavení promptu s historií\n",
    "        history_prompt_parts = []\n",
    "        for user_msg, assistant_msg in history:\n",
    "            history_prompt_parts.append(f\"[INST] {user_msg} [/INST] {assistant_msg}</s>\")\n",
    "        history_prompt = \"\".join(history_prompt_parts)\n",
    "        final_prompt = f\"<s>{history_prompt}[INST] {message} [/INST]\"\n",
    "\n",
    "        # Tokenizace a zjištění délky vstupu\n",
    "        inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_token_len = inputs.input_ids.shape[1]\n",
    "\n",
    "        # Generování odpovědi\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Robustní dekódování pouze nově vygenerovaných tokenů\n",
    "        generated_tokens = outputs[0, input_token_len:]\n",
    "        clean_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return clean_response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Pokud cokoliv selže, vytiskneme chybu do výstupu buňky\n",
    "        print(\"\\n❌ CHYBA BĚHEM GENEROVÁNÍ ODPOVĚDI V GRADIO FUNKCI!\")\n",
    "        traceback.print_exc()\n",
    "        return \"Omlouvám se, došlo k interní chybě. Podívejte se na výstup v Jupyter buňce pro detaily.\"\n",
    "\n",
    "print(\"✅ Chatovací funkce 'chat_with_model_gradio' je definována.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chat je připraven! Můžete začít psát.\n",
      "Napište 'konec' nebo 'exit' pro ukončení chatu.\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent: None\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistent přemýšlí...\n",
      "\n",
      "AI Asistent: None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Buňka 6 (Alternativa): Interaktivní chat přímo v notebooku\n",
    "\n",
    "# Seznam pro uchování historie konverzace (důležité pro kontext)\n",
    "chat_history = []\n",
    "\n",
    "print(\"✅ Chat je připraven! Můžete začít psát.\")\n",
    "print(\"Napište 'konec' nebo 'exit' pro ukončení chatu.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    # 1. Získání vstupu od uživatele\n",
    "    user_input = input(\"Vy: \")\n",
    "    if user_input.lower() in [\"konec\", \"exit\", \"quit\"]:\n",
    "        print(\"Chat ukončen.\")\n",
    "        break\n",
    "        \n",
    "    # 2. Sestavení promptu s historií\n",
    "    history_prompt_parts = []\n",
    "    for user_msg, assistant_msg in chat_history:\n",
    "        # Formát pro Mistral-Instruct\n",
    "        history_prompt_parts.append(f\"[INST] {user_msg} [/INST] {assistant_msg}</s>\")\n",
    "    history_prompt = \"\".join(history_prompt_parts)\n",
    "    \n",
    "    final_prompt = f\"<s>{history_prompt}[INST] {user_input} [/INST]\"\n",
    "    \n",
    "    # 3. Tokenizace a příprava vstupu pro model\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_token_len = inputs.input_ids.shape[1]\n",
    "    \n",
    "    print(\"\\nAsistent přemýšlí...\")\n",
    "    \n",
    "    # 4. Generování odpovědi\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # 5. Dekódování a vyčištění odpovědi\n",
    "    generated_tokens = outputs[0, input_token_len:]\n",
    "    clean_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    print(f\"\\nAI Asistent: {clean_response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 6. Aktualizace historie konverzace\n",
    "    chat_history.append((user_input, clean_response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
