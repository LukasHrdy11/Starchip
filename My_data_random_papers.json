[
  {
    "topic": "Quantum Computing > Neutral-Atom Quantum Computing > Overview",
    "question": "What are the key advantages of Neutral-Atom Quantum Computing (NAQC) as a hardware platform, and what primary challenges arise when compiling quantum circuits, particularly in comparison to superconducting qubit architectures?",
    "context": "Neutral-atom quantum computing (NAQC) offers distinct advantages such as dynamic qubit reconfigurability, long coherence times, and high gate fidelities, making it a promising platform for scalable quantum computing. Despite these strengths, efficiently implementing quantum circuits like the Quantum Fourier Transform (QFT) remains a significant challenge due to atom movement overheads and connectivity constraints. This paper introduces optimal compilation strategies tailored to QFT circuits and NAQC systems, addressing these challenges for both linear and grid-like architectures. By minimizing atom movements, the proposed methods achieve theoretical lower bounds in atom movements while preserving high circuit fidelity. Comparative evaluations against state-of-the-art compilers demonstrate the superior performance of the proposed methods. These methods could serve as benchmarks for evaluating the performance of NAQC compilers.\n\nQuantum computing has emerged as a transformative technology capable of solving complex problems in cryptography¹, database search², chemical simulations³, and machine learning⁴. The rapid advancement of quantum computing research has led to the development of multiple hardware platforms, each with unique strengths and challenges. Among these, superconducting qubits have gained prominence due to their robust control schemes and compatibility with conventional microelectronics⁵. In the past several years, neutral-atom quantum computing (NAQC) has attracted significant attention for its inherent advantages, including scalability, high qubit connectivity, long coherence times, and superior gate fidelity⁶⁻⁸.\n\nIn superconducting quantum devices, such as IBM's heavy-hex architecture, qubits are sparsely arranged to minimize crosstalk. This sparse arrangement, however, requires additional routing steps (mainly through inserting SWAP gates) to facilitate operations between distant qubits. In contrast, NAQC devices, often arranged on a 2D grid, enable two-qubit gates between any two qubits by shuttling them closer. However, longer shuttling distances can increase noise levels. Thus, careful circuit compilation is critical on both platforms to achieve high circuit fidelity.",
    "answer": "Neutral-Atom Quantum Computing (NAQC) is a promising platform for scalable quantum computing due to its distinct advantages: dynamic qubit reconfigurability, long coherence times, high gate fidelities, scalability, and high qubit connectivity. Unlike superconducting qubit architectures, which often feature sparse qubit arrangements to minimize crosstalk, NAQC devices (typically arranged on a 2D grid) enable two-qubit gates between any two qubits by physically shuttling atoms closer. This inherent mobility eliminates the need for costly SWAP gates, which are commonly required in fixed-architecture superconducting devices to facilitate interactions between non-adjacent qubits. However, this unique capability of NAQC also introduces a primary compilation challenge: minimizing atom movement overhead. Longer shuttling distances in NAQC can increase noise levels, making careful circuit compilation crucial to maintain high circuit fidelity by optimizing atom movements and addressing connectivity constraints."
  },
  {
    "topic": "Quantum Computing > Neutral-Atom Quantum Computing > DPQA Architecture",
    "question": "Describe the step-by-step execution procedure for Controlled-Z (CZ) gates in a Dynamically Field-Programmable Qubit Array (DPQA) architecture within Neutral-Atom Quantum Computing, including the distinct modes of the 'separate' step and their implications for atom movement.",
    "context": "2.4 Compilation Procedures in NAQC \nConsider a quantum circuit C consisting of single- and two-qubit gates that are native to a DPQA device M. In NAQC, single-qubit gates are implemented independently using Raman laser pulses and are therefore typically excluded from C when assessing the cost of atom movement. This simplification allows the problem to focus on the native two-qubit gates, which are typically CZ gates. These CZ gates are organized into layers, each of which can, in principle, be executed in parallel through global Rydberg illumination. \nInitially, each logical qubit in C is mapped to a physical atom in the DPQA device M. For each layer of parallel CZ gates, any pair of interacting qubits must be co-located at the same site (i.e., grid point) during the gate operation and separated afterward. As illustrated in Fig. 2, the execution of any CZ gate (or several CZ gates in parallel) involves the following steps: \n1. Meet: Move one or both atoms to the same site from their previous positions. \n2. Interact: Apply a global Rydberg laser to interact the qubits at the same position, executing the corresponding CZ gates in parallel. \n3. Separate: After the interaction, move the atoms apart so that they occupy different sites. \nThe execution of each CZ involves at least two atom moves: a 'meet' step and a \"separate\" step. These processes can often be parallelized across several or all gates within the same layer. The maximum distance moved during these steps determines the optimal distance required. In the DPQA device M, this maximal distance is at least d in each step, where d is the unit distance of the device. Consequently, a circuit with S layers of CZ gates requires at least 2S atom moves, covering a total distance of at least 2Sd. \nWe note that the “separate\" step is highly flexible, a flexibility that plays a crucial role in this paper. Specifically, we distinguish between two modes of this step: the “swap” mode exchanges the positions of the two interacting atoms, and the “restore\" mode, which returns the two interacting atoms to their original positions. See Figure 2 for illustrations of these modes.",
    "answer": "In a Dynamically Field-Programmable Qubit Array (DPQA) architecture, the execution of Controlled-Z (CZ) gates, which are the native two-qubit operations, is organized into layers that can be executed in parallel using global Rydberg illumination. The process involves three primary steps for each CZ gate or parallel batch of gates:\n\n1.  **Meet:** One or both atoms involved in the gate are moved from their previous physical positions to the same target grid point to enable interaction.\n2.  **Interact:** A global Rydberg laser is applied, causing the co-located qubits at the shared grid point to interact and execute the corresponding CZ gates in parallel.\n3.  **Separate:** Following the interaction, the atoms are moved apart to occupy distinct grid points.\n\nEach CZ gate execution necessitates at least two atom movements: one for the 'meet' step and one for the 'separate' step. These movements can be parallelized across multiple gates within the same layer. The maximum distance an atom moves during these steps, denoted by \\(d\\) (the unit distance of the device), dictates the optimal distance required. A circuit with \\(S\\) layers of CZ gates thus requires at least \\(2S\\) atom moves, covering a total distance of at least \\(2Sd\\).\n\nThe 'separate' step offers flexibility with two distinct modes:\n*   **Swap Mode:** The positions of the two interacting atoms are exchanged. This is particularly useful for simulating SWAP operations without additional gate overhead.\n*   **Restore Mode:** The interacting atoms are returned to their original positions prior to the 'meet' step.\n\nThis flexibility in the 'separate' step is crucial for optimizing circuit compilation by strategically minimizing atom movement overhead."
  },
  {
    "topic": "Quantum Computing > Neutral-Atom Quantum Computing > QFT Compilation Strategies > Linear Path",
    "question": "How does the Linear Path strategy achieve optimal compilation for Quantum Fourier Transform (QFT) circuits on a linear Neutral-Atom Quantum Computing (NAQC) architecture, and what specific steps are involved for each m-stage to minimize atom movements?",
    "context": "4.2 Optimal Transformation in a Linear DPQA Architecture \nIn this subsection, we propose an optimal transformation for QFT circuits on a linear DPQA architecture, which can be viewed as a 1 × n grid. Each grid point is represented by its coordinate P₁ = (i,0), and the unit distance between consecutive grid points is d. \nThe transformation builds on Maslov's approach for linear superconducting architectures, where SWAP gates are inserted to enable interactions between non-adjacent qubits (see Fig. 3c). In DPQA, we replace these SWAP gates with atom movements, which are more efficient. Specifically, for each m-stage (mapping stage) in Maslov's transformation, the layer of controlled-phase gates is replaced with two identical layers of CZ gates. We then perform the following steps for each m-stage: \na. First CZ-layer: \n• Move the atoms involved in the gates to the same grid point (meet step). \n• Apply the gates using a global Rydberg laser (interact step). \n• Move the atoms back to their original positions (separate step in restore mode). \nb. Second CZ-layer: \n• Move the atoms involved in the gates to the same grid point (meet step). \n• Apply the gates using a global Rydberg laser (interact step). \n• Swap the positions of the atoms (separate step in swap mode). \nThis approach ensures that all CZ gates are executed with minimal atom movements, achieving the theoretical lower bound of 4(2n-3)d. The transformation is optimal because it parallelizes movements within each m-stage and minimizes the total distance traveled by the atoms. \nMore precisely, for edges ej = (j, j + 1) with j ∈ Ek (as defined in (2)), the atom movements are performed as follows: in Step a, we move all smaller-indexed atoms towards the larger-indexed atoms and move them back; in Step b, we move all smaller-indexed atoms towards the large-indexed atoms and swap their positions. \nThe movement directions are consistent within each batch of atom movements, and each move covers only a unit distance d. This ensures that the total maximal distance traveled by the atoms is 4(2n – 3)d, which matches the theoretical lower bound. Therefore, the transformation on the linear DPQA architecture is optimal. \nAs a consequence, we have the following result: \nTheorem 1. Let M be a linear DPQA architecture of n qubits. The transformation described above for QFT-n is an optimal transformation on M. \nTo illustrate the transformation, we revisit the QFT-5 circuit. \nExample 1. The QFT-5 circuit consists of 7 m-stages and 14 layers of CZ gates. Initially, the logical qubits qi are mapped to the atom located at the grid point P₁ = (i, 0) for 0 ≤ i ≤ 4 on the linear architecture. Fig. 4 illustrates the mapping stages and movement sequences for QFT-5. \nFor example, at m-stage 3, the set of edges E3 = { (1,2), (3,4) } indicates that atom movements are performed along these edges. Specifically: \n• In the first layer of CZ gates (Step a), the atoms at P₁ and P3 are moved (along the same direction), respectively, to P2 and P4, the CZ gates are applied, and the atoms are returned to their original positions. \n• In the second layer of CZ gates (Step b), the atoms at P₁ and P3 are moved (along the same direction), respectively, to P2 and P4, the CZ gates are applied, and their positions are swapped. \nThe red edges in Fig. 4 highlight the atom movements along the edges e ∈ Ek for each m-stage k (where 0 ≤ k ≤ 6), demonstrating how the transformation achieves the theoretical lower bound on movement counts.",
    "answer": "The Linear Path strategy achieves optimal compilation for Quantum Fourier Transform (QFT) circuits on a linear Neutral-Atom Quantum Computing (NAQC) architecture (modeled as a \\(1 \\times n\\) grid) by replacing SWAP gates from Maslov's classical approach with atom movements. This is achieved by decomposing each controlled-phase gate into two Controlled-Z (CZ) gates, resulting in \\(2(2n-3)\\) CZ layers for an \\(n\\)-qubit QFT circuit.\n\nFor each m-stage in Maslov's transformation, the strategy involves two distinct CZ-layers:\n\na.  **First CZ-layer:**\n    *   **Meet:** The atoms involved in the gates are moved to their respective interaction grid points.\n    *   **Interact:** A global Rydberg laser is applied to execute the CZ gates in parallel.\n    *   **Separate:** The atoms are moved back to their original positions (restore mode).\n\nb.  **Second CZ-layer:**\n    *   **Meet:** The atoms involved in the gates are again moved to their respective interaction grid points.\n    *   **Interact:** A global Rydberg laser is applied to execute the CZ gates in parallel.\n    *   **Separate:** The positions of the atoms are swapped (swap mode).\n\nThis method ensures that all CZ gates are executed with minimal atom movements, specifically achieving the theoretical lower bound of \\(4(2n-3)d\\) total distance traveled by atoms, where \\(d\\) is the unit distance. The optimality stems from parallelizing movements within each m-stage and ensuring each move covers only a unit distance, thus minimizing overall atom transport and maintaining high circuit fidelity."
  },
  {
    "topic": "Quantum Computing > Neutral-Atom Quantum Computing > QFT Compilation Strategies > Zigzag Path",
    "question": "Explain the Zigzag Path strategy for optimally compiling QFT circuits on grid-like DPQA architectures, including the definition of zigzag folding, its key properties, and the atom movement procedures for each m-stage.",
    "context": "4.3 Optimal Transformation on a Grid Architecture \nWhile the linear architecture achieves minimal movement cost, it requires qubits to be arranged in a one-dimensional array, which becomes impractical for large numbers of qubits. To address this, we propose folding the linear architecture into a two-dimensional grid while preserving the efficiency of the linear transformation. This is achieved through a technique called zigzag folding, which maps the linear arrangement of qubits onto a grid in a way that maintains adjacency and enables parallel gate execution. \nDefinition 1 (Zigzag Folding). Let P be a linear DPQA architecture of n qubits 90,91,...,qn−1, and let M be a DPQA architecture with grid dimensions m₁ × m2. A zigzag folding I of P into M is an injective mapping that assigns a unique grid point (xi, yi) to each qubit qi such that: \n• Neighboring Placement: Ф(qi) is a neighbor of Ф(qi−1) in M, meaning |xi – Xi−1| + |Yi - Yi−1| = 1. \n• Alternating Direction: If qi is placed horizontally relative to qi−1, then qi+1 (if i < n − 1) is placed vertically relative to qi, and vice versa. \nA zigzag folding ensures that neighboring qubits in the linear arrangement remain adjacent on the grid, enabling efficient gate execution. By Lemma 2, the gate operations and atom movements in each m-stage correspond to either horizontal or vertical edges in the zigzag folding, allowing for parallel execution. \nTheorem 2. Let Þ be a zigzag folding of a linear DPQA architecture of n qubits into a grid DPQA architecture M. Then QFT-n has an optimal transformation on M as follows: \n1. Initially, map each qi to Þ(qi). \n2. For each m-stage k, let Ek be the set of relevant edges as specified in (2). If the edges in Ek are horizontal (vertical, resp.): \n• For the first layer of CZ gates in the m-stage, move all left (lower, resp.) atoms rightward (upward, resp.), execute the gates using a global Rydberg laser, and then separate the atoms by moving them back. \n• For the second layer of CZ gates in the m-stage, move all left (lower, resp.) atoms rightward (upward, resp.), execute the gates, and then separate the atoms by exchanging their positions. \nProof. Without loss of generality, assume that q1 is placed as the right neighbor of qo in the zigzag folding Ф. By the definition of zigzag folding, the placement of qubits alternates between horizontal and vertical directions. Consequently: \n• For even k, all edges in Ek are horizontal. \n• For odd k, all edges in Ek are vertical. \nFor each m-stage k, the transformation involves four batches of atom movements: \n• Two batches for the first layer of CZ gates (Step a). \n• Two batches for the second layer of CZ gates (Step b). \nSince all movements within a batch occur in the same direction (either horizontal or vertical), they can be performed in parallel. Furthermore, each movement covers a unit distance d, ensuring that the total distance traveled by the atoms is minimized. The maximal movement distance for the entire transformation is 4(2n – 3)d, which matches the theoretical lower bound. This is achieved because: \n• Each of the 2(2n - 3) layers of CZ gates requires two atom movements (one for the meet step and one for the separate step). \n• Each movement covers a unit distance d. \n• Movements are parallelized within each m-stage, ensuring no unnecessary overhead.",
    "answer": "The Zigzag Path strategy extends optimal QFT compilation from linear to grid-like DPQA architectures by folding the one-dimensional qubit arrangement into a two-dimensional grid. This `zigzag folding` is an injective mapping, \\(\\Phi\\), that assigns each logical qubit \\(q_i\\) to a unique grid point \\((x_i, y_i)\\) in the \\(m_1 \\times m_2\\) grid \\(M\\), maintaining two key properties:\n\n1.  **Neighboring Placement:** Adjacent qubits in the linear arrangement remain neighbors on the grid, meaning \\(|x_i - x_{i-1}| + |y_i - y_{i-1}| = 1\\).\n2.  **Alternating Direction:** The placement alternates between horizontal and vertical directions; if \\(q_i\\) is placed horizontally relative to \\(q_{i-1}\\), then \\(q_{i+1}\\) is placed vertically relative to \\(q_i\\), and vice versa.\n\nThis folding ensures that gate operations and atom movements within each m-stage correspond to either purely horizontal or purely vertical edges, allowing for efficient parallel execution. For each m-stage \\(k\\), the transformation procedure is:\n\n1.  **Initial Mapping:** Each logical qubit \\(q_i\\) is initially mapped to its corresponding grid point \\(\\Phi(q_i)\\).\n2.  **CZ Gate Layers:** For the set of relevant edges \\(E_k\\) (which are either all horizontal for even \\(k\\) or all vertical for odd \\(k\\)):\n    *   **First Layer (Restore Mode):** Atoms on the 'left' (or 'lower' for vertical edges) side of the interaction are moved 'rightward' (or 'upward') to co-locate with their partners. After the CZ gates are applied via global Rydberg illumination, the atoms are moved back to their original pre-meet positions.\n    *   **Second Layer (Swap Mode):** Similar 'meet' and 'interact' steps occur. However, in the 'separate' step, the atoms exchange their positions, effectively performing a SWAP operation without extra gate overhead.\n\nThis strategy, by ensuring parallel movements and unit distance travel for each atom move, achieves the theoretical lower bound of \\(4(2n-3)d\\) for the total maximal movement distance, thereby providing an optimal QFT compilation on grid-like DPQA architectures."
  },
  {
    "topic": "Quantum Computing > Neutral-Atom Quantum Computing > Performance Evaluation",
    "question": "What are the three key performance metrics used to evaluate quantum circuit compilation strategies in Neutral-Atom Quantum Computing, and how is the 'Overall Fidelity' calculated based on various error sources?",
    "context": "5.2 Key Performance Metrics \nWe evaluate these methods based on three critical metrics: \n(1) Big Move Counts: The number of significant positional shifts of qubits during circuit execution. Minimising this reduces operational overhead and potential errors. \n(2) Cumulative Movement Distances: The total distance qubits travel, including both big moves and short offsets executed to resolve placement conflicts (i.e., preventing qubits from occupying the same physical location). \n(3) Overall Fidelity: Calculated based on errors from two-qubit gate error, global Rydberg excitation error, atom transfer error, and decoherence error, following the fidelity model outlined in 22: \nf = (f2)82 \n× (fexc) 2S-282 x (ftrans) Ntrans \nX \n1 \nTa \nT2 \n-------- \nqEQ \ntwo-qubit gate Rydberg excitation \natom transfer \ndecoherence \n(3) \nwhere: \n• f2 is the two-qubit gate fidelity. \n• g2 is the number of CZ gates in the compiled circuit. Atomique may insert SWAP gates and each SWAP gate incurs three CZ gates. \n• Q is the qubit set in the compiled circuit. Atomique may use ancilla qubits. \n• S is the number of Rydberg stages. Atomique may use more stages than the other compilers, which all use a minimal number of stages. \n• fexc is the fidelity of an isolated (i.e., not interacting with another qubit) qubit in a Rydberg stage. For DasAtom, fexc = 1 as it employs individually addressible Rydberg lasers. \n• ftrans measures fidelity losses from atom transfers. Atomique uses no atom transfers. \n• Ntrans is the number of atom transfers. Atomique uses no atom transfers. \n• Tq is the idling time for qubit q. \n• T2 is the coherence time. \nTable 1 lists the key parameters adopted in our experiments, which are also assumed in22. \nTable 1. Key DPQA/NAQC parameters, where d represents the default spacing (i.e., unit distance) in the SLM array, and a denotes the acceleration of qubit movement.",
    "answer": "Quantum circuit compilation strategies in Neutral-Atom Quantum Computing (NAQC) are evaluated using three critical performance metrics:\n\n1.  **Big Move Counts:** This metric quantifies the total number of significant positional shifts of qubits during circuit execution. Minimizing this reduces operational overhead and potential errors.\n2.  **Cumulative Movement Distances:** This metric measures the total distance traveled by all atoms, encompassing both large-scale 'big moves' and smaller 'short offsets' necessary to resolve qubit placement conflicts. This distance is typically measured in grid units, where the unit distance \\(d\\) represents the default spacing in the SLM array.\n3.  **Overall Fidelity:** This metric provides a comprehensive assessment of circuit quality by accounting for various error sources. It is calculated using the following model:\n    \\(f = (f_2)^{g_2} \\times (f_{exc})^{2S-2g_2} \\times (f_{trans})^{N_{trans}} \\times \\prod_{q \\in Q} (1 - \\frac{T_q}{T_2})\\)\n    Where:\n    *   \\(f_2\\) is the two-qubit gate fidelity.\n    *   \\(g_2\\) is the total number of CZ gates in the compiled circuit.\n    *   \\(f_{exc}\\) is the fidelity of an isolated qubit in a Rydberg stage (for DasAtom, \\(f_{exc}=1\\) due to individually addressable lasers).\n    *   \\(S\\) is the number of Rydberg stages.\n    *   \\(f_{trans}\\) measures fidelity losses from atom transfers.\n    *   \\(N_{trans}\\) is the total number of atom transfers.\n    *   \\(T_q\\) is the idling time for a specific qubit \\(q\\).\n    *   \\(T_2\\) is the coherence time.\n    *   \\(Q\\) is the set of qubits in the compiled circuit, where some compilers (e.g., Atomique) may use ancilla qubits."
  },
  {
    "topic": "Quantum Computing > Neutral-Atom Quantum Computing > Performance Evaluation > Fidelity Analysis",
    "question": "Compare the overall fidelity performance of Linear Path, Zigzag Path, Enola, Atomique, and DasAtom compilation strategies for Quantum Fourier Transform (QFT) circuits, specifically detailing their relative fidelity achievements and highlighting the factors contributing to observed fidelity gaps for varying qubit counts.",
    "context": "5.3.3 Overall Fidelity Analysis \nFig. 6c compares the overall fidelity achieved by each method, where the TLB is calculated based on four atom transfers per CZ gate. \n• Linear & Zigzag Paths achieve the highest fidelity, as their movement efficiency minimises errors from atom transfers and decoherence. \n• Enola's methods show exponentially lower fidelity, reflecting their higher movement counts and distances. \n• Atomique experiences fidelity degradation as circuit size increases, primarily due to its reliance on SWAP gates and ancilla qubits. \n• DasAtom achieves higher overall fidelity than the TLB by leveraging individually addressable Rydberg lasers and long-range interactions. Since it employs local two-qubit operations without requiring global Rydberg illumination, it avoids global Rydberg excitation error. Effectively, this is equivalent to setting fexc = 1 for DasAtom in (3). \nSince the first three terms in (3) are identical for TLB, Line and Zigzag Paths, and Enola, the overall fidelity for these compilers is predominantly determined by big move counts and cumulative movement distances. \nQuantifying Fidelity Gaps. Table 2 highlights the stark fidelity gaps in larger QFT circuits. For n = 30, Enola's fidelity is more than 180× lower than our Linear Path, while Atomique's is over 1018 × lower. At n = 50, these gaps widen dramatically to factors of 1015 and 1094, respectively, emphasizing the critical role of movement minimization in large-scale circuits. \nMeanwhile, the table suggests that with individually addressable Rydberg lasers, DasAtom's fidelity could surpass the TLB by more than 64× for QFT-30 and 106× for QFT-50. This advantage persists even when excitation error is ignored for the TLB, where DasAtom's fidelity remains nearly 10× higher for QFT-30 and over 200× higher for QFT-50. This improvement stems from DasAtom's use of long-range interactions. However, we note that current state-of-the-art NAQC with individually addressed Rydberg lasers faces limitations in two-qubit gate fidelity28. \nTable 2. Comparison on QFT-30 and QFT-50, where distances are measured in unit grid distance (d = 3µm for DasAtom and d = 15µm for all the other compilers). In Atomique, this distance is not computed by the code. Note that excitation error does not impact DasAtom's fidelity, as it utilizes local Rydberg lasers. \nCircuit Method g2 Q S Ntrans Big Move count | distance Offset Move count distance Overall Fidelity fexc = 0.9975 | fexc=1 \nTLB 870 30 114 3478 227 227 0 0 5.7 x 10-5 3.8 x 10-4 \nLinear 870 30 114 3478 227 226.84 681 83.53 2.1 x 10-6 1.4 x 10-5 \nZigzag 870 30 114 3478 227 226.84 1413 178.47 1.7 x 10-6 1.1 x 10-5 \nQFT-30 Enola 870 30 114 3478 813 1253.37 4681 669.84 1.2 × 10-8 7.9 x 10-7 \nAtomique 954 43 466 0 237 805.45 229 2.9 x 10-25 1.5 x 10-5 \nDasAtom 870 30 786 460 106 243.21 640 68.42 3.7 x 10-3 3.7 x 10-3 \nTLB 2450 50 194 9798 387 387 0 0 1.4 x 10-15 2.4 x 10-10 \nLinear 2450 50 194 9798 387 386.84 1161 142.47 8.3 × 10-17 1.4 x 10-11 \nZigzag 2450 50 194 9798 387 386.84 3501 452.58 2.9 × 10-18 5.0 × 10-13 \nQFT-50 Enola 2450 50 194 9798 1922 4050.79 13318 1910.84 1.1 x 10-31 1.8 × 10-26 \nAtomique 2669 79 1145 0 819 4000.29 326 6.6 × 10-110 2.2 x 10-17 \nDasAtom 2450 50 1554 1264 221 567.16 1701 182.05 5.2 x 10-8 5.2 x 10-8",
    "answer": "For Quantum Fourier Transform (QFT) circuits, the Linear Path and Zigzag Path strategies demonstrate superior overall fidelity by minimizing atom transfer errors and decoherence through efficient movement. In contrast:\n\n*   **Enola's methods** exhibit exponentially lower fidelity due to significantly higher movement counts and distances. For \\(n=30\\), Enola's fidelity is over 180 times lower than Linear Path, and for \\(n=50\\), this gap widens to \\(10^{15}\\) times, emphasizing the critical role of movement minimization.\n*   **Atomique** shows fidelity degradation as circuit size increases, largely attributed to its reliance on SWAP gates and the use of ancilla qubits, which introduce additional errors.\n*   **DasAtom** achieves higher overall fidelity than the theoretical lower bound (TLB) by leveraging individually addressable Rydberg lasers and long-range interactions. This approach avoids global Rydberg excitation error, effectively setting \\(f_{exc}=1\\) in the fidelity model. For QFT-30, DasAtom's fidelity is over 64 times higher than the TLB, and for QFT-50, it's over 106 times higher. This advantage persists even when excitation error is neglected for the TLB, showing DasAtom's fidelity remains significantly higher. However, it's noted that current state-of-the-art NAQC with individually addressed Rydberg lasers still faces limitations in two-qubit gate fidelity.\n\nThe overall fidelity for TLB, Linear, Zigzag Paths, and Enola is primarily determined by their big move counts and cumulative movement distances, as the first three terms of the fidelity formula are identical for these compilers."
  },
  {
    "topic": "Quantum Computing > Use Case Evaluation > ITBQ Framework",
    "question": "What is the ITBQ (Identify, Transform, Benchmark, Show Quantum Advantage) framework for evaluating quantum computing use cases, and what specific challenges does it highlight in demonstrating quantum advantage, particularly concerning the 'Transform to Quantum' and 'Get the Job Done Without the Quantum Computer' criteria?",
    "context": "What is a good use case for quantum computers? \nMichael Marthaler, Peter Pinski, Pascal Stadler, Vladimir Rybkin, Marina Walt \nHQS Quantum Simulations GmbH, Rintheimer Str. 23, 76131 Karlsruhe, Germany \nIdentify, Transform, Benchmark, Show Quantum Advantage (ITBQ): Evaluating use cases for quantum computers. We introduce a four-step framework for assessing quantum computing applications from identifying relevant industry problems to demonstrating quantum advantage addressing steps often overlooked in the literature, such as rigorous benchmarking against classical solutions and the challenge of translating real-world tasks onto quantum hardware. Applying this framework to cases like NMR, multireference chemistry, and radicals reveals both significant opportunities and key barriers on the path to practical advantage. Our results highlight the need for transparent, structured criteria to focus research, guide investment, and accelerate meaningful quantum progress. \nI. INTRODUCTION \nQuantum computing hardware has progressed substan-tially in the last few years. Several quantum comput-ing hardware providers can relatively consistently achieve two qubit gates with more than 99% Fidelity. Under lab-oratory conditions, even 99.9% has been achieved in quite a few architectures, and at least one cloud-accessible de-vice with these remarkable fidelities does exist (Quantin-uum). This all makes it even more important to drive use cases forward. A vast amount of work has been dedicated to testing various algorithms and approaches in the last few years [1-6] and it is now all the more important to assess the state and quality of the existing ideas for use cases. \nIn this work, we want to discuss the quality and current state of some exemplary use cases using four interdepen-dent criteria: \n• Identify industry problem, \n• Transform to quantum, \n• Get the job done without the quantum computer, \n• Show quantum advantage.\"\n\n\"Apart from reviewing some important historic use cases and three use cases from our own team, we intend to add two important criteria for assessing quantum com-puting use cases. One criterion – Transform to Quantum is about the necessity to consider the step of transform-ing the actual input as you get it from an end user into the formulation required for the quantum computer. A key aspect here is that only a small subset of the prob-lem might benefit from using the quantum computer and needs to be efficently extracted from a larger problem. When evaluating the quality of current use cases, partic-ular attention should be given to the state of research and the availability of software to perform this transformation step. This step can present a significant barrier to effec-tive use case implementation if the software required to identify and extract the most relevant subproblem is not available. At the same time, it can serve as the crucial step to connect otherwise abstract quantum algorithms to real end users. \nThe second criterion we intend to add is the need to invest as much effort to solve the given problem with clas-sical computers as into the research on quantum comput-ing. We call this criteria Get the Job Done Without the Quantum Computer. Of course, scientific research often has limited ressources, so it might not always be possi-ble to fully optimize classical methods before exploring the quantum ones. Nevertheless, at some stage in the process, it's important to consider the status and avail-ability of optimized classical solutions when judging any work on a quantum use case. As an example, one of the first claims advanced for an andvantage in using quan-tum computers was based on a benchmark performed by a D-Wave quantum computer [8] versus classical CPLEX solver, a standard tool for combinatorial optimization problems. However, this example did not stand up to scrutiny in the long run, since CPLEX is not the best possible solver for the type of problem addressed by the D-Wave architecture [9]. In another recent instance, an \nimpressive large-scale simulation of spin dynamics on a quantum computer was reproduced by several groups us-ing classical computers in a matter of weeks [10]. There-fore, the question if the problem has been compared to optimized classical solvers is always quite important to assess a quantum computing use case.\"\n\n\"These four steps - Identify, Transform, Benchmark, Show Quantum Advantage (ITBQ) – are obviously also interconnected (Fig. 1). Once the relevant industry prob-lem has been identified, one needs to explore if optimized classical solvers are also available. When classical solvers are sufficient to solve the identified industry problem, the quantum advantage for the actual problem is brought into question. In particular, if this is possible for a large number of examples (for example, different molecules) for the given use case, either the scope of the existing problem needs to be revisited, or the possibility of quan-tum advantage is to be considered improbable. This demonstrates that benchmarking against optimized clas-sical solvers, particularly those utilizing problem-specific approximations, often presents a significant obstacle to finding use cases for quantum computers. But at the same time finding a classical solution to achieve a satis-factory solution has obviouly also a substantial value for any end user.\"\n\n\"On the more possitive note for quantum advantage, the possibility to find broadly applicable use cases for quantum computing can be substanially enhanced by investing efforts of transforming the original industry problem into a formulation appropriate for the quantum computer. As an example, solving differential equations is probably one of the key promising problems in ap-plications of quantum computing. It has been shown that in principle we expect quantum computers to be able to solve differential equation with an exponential speedup [11-13]. But, a differential equation is never the actual industry problem. Usually, the actual problem re-quires the solutions of a differential equation. However, at this point only a small amount of work is published on the question how to transform the actual problem \ninto the quantum computer without loosing the expected speedup. To the best of our knowledge, reference [14] is among the only studies addressing this topic directly in the context of manufacturing engineering and some work on problem-specific differential equations for ma-chine learning [15, 16]. And much more should be possi-ble.\"\n\n\"In Table I we introduce our rating for these four ITBQ criteria and provide further details how to rank the use cases to track the process towards demonstrat-ing quantum advantage. This table can be understood as a checklist and we will use the rating when describ-ing the use cases in the following chapters.\"\n\n\"The overall landscape, as highlighted in recent bench-marking studies [17], is that while industrial optimiza-tion problems are of truly exceptional practical relevance, current approaches of transforming them to quantum and benchmarking for quantum advantage remain imma-ture [18-20]. Neither a uniformally established method-ology exists to fairly demonstrate quantum edge on in-dustrially meaningful optimization tasks, nor is there full maturity in the quantum-classical software toolchain that would make real-world adoption straightforward.\"\n\n\"Despite the significant interest in applying quantum computing to industrial optimization problems, current use case studies reveal a lack of provable quantum ad-vantage, whether exponential or polynomial. While it is anticipated that quantum algorithms may offer a polyno-mial speedup, this may prove insufficient given the slow gate times associated with error-corrected quantum com-puters [21]. Recent research on the Optimal Polynomial Intersection (OPI) problem provides at least some evi-dence, that exponential speedup could be achievable for finding approximate solutions [22]. However, it is im-portant to note that the OPI problem does not directly \nIn Table I we introduce our rating for these four ITBQ criteria and provide further details how to rank the use cases to track the process towards demonstrat-ing quantum advantage. This table can be understood as a checklist and we will use the rating when describ-ing the use cases in the following chapters. After dis-cussing the two contrasting archetypes – Industrial Opti-mization and Abstract Spin Problems – in section II, we will discuss three problems at different levels of readiness to demonstrate quantum advantage: Nuclear Magnetic Resonance in sections III A, Multireference Chemistry in section III C and Radicals in section IIID. Finally, we summarize our assessment for the discussed problems and conclude on the interpretation in section IV.\" \n\n\"II. TWO CONTRASTING ARCHETYPES: \nINDUSTRIAL OPTIMIZATION AND ABSTRACT \nSPIN PROBLEMS \nAs the field of quantum computing matures, con-siderable attention has turned to the quest for \"quan-tum advantage\" in real-world applications. Two espe-cially prominent families of use cases are the solution of industry-critical optimization problems and the simula-tion of abstract spin models, each highlighting key obsta-cles and open questions. Examining these two archetypes sharpens our understanding of both the potential and the current limitations in the search for valuable quantum applications.\"",
    "answer": "The ITBQ (Identify, Transform, Benchmark, Show Quantum Advantage) framework is a four-step methodology for evaluating quantum computing use cases, aiming to bridge the gap between theoretical quantum algorithms and practical real-world applications. The four interdependent criteria are:\n\n1.  **Identify industry problem:** Pinpointing a genuinely interesting and industrially relevant problem.\n2.  **Transform to quantum:** Converting the real-world input into a format suitable for quantum computation.\n3.  **Get the job done without the quantum computer:** Rigorously benchmarking against and optimizing classical solutions.\n4.  **Show quantum advantage:** Demonstrating a provable and significant speedup or benefit using quantum resources.\n\nThe framework highlights several challenges in achieving quantum advantage:\n\n*   **'Transform to Quantum':** This step often requires extracting only a quantum-suitable subset of a larger problem, which can be difficult and lacks robust, automated software tools. Many industrial problems are high-dimensional and have diverse constraints, making the transformation non-straightforward. Failure to efficiently transform the problem without losing the expected speedup creates a significant barrier to connecting abstract quantum algorithms to end-users.\n*   **'Get the Job Done Without the Quantum Computer':** There's a critical need to invest as much effort into optimizing classical methods as into quantum research. Without fair and rigorous benchmarking against state-of-the-art classical solvers (which may continue to improve), claims of quantum advantage can be undermined. For instance, early claims based on D-Wave quantum computers were scrutinized because the classical solver used was not the best available.\n\nOverall, while quantum computing hardware has advanced significantly, the ITBQ framework emphasizes that successfully demonstrating quantum advantage for real-world problems remains a complex and immature endeavor, particularly in areas like industrial optimization, where convincing evidence of quantum advantage is still elusive, despite the high practical relevance."
  },
  {
    "topic": "Quantum Computing > Use Cases > Nuclear Magnetic Resonance (NMR)",
    "question": "How is Nuclear Magnetic Resonance (NMR) spectroscopy identified as a quantum computing use case, what are the key parameters in its Spin Hamiltonian, and what is the current state of classical and quantum approaches for its simulation?",
    "context": "A. Nuclear Magnetic Resonance \nradio frequency coil \npulses \n(Larmor frequency) \nstrong magnetic field \n→ time \nFIG. 4. Principles of an NMR spectrometer: the sample is placed in a strong magnetic field, causing a net magnetization of the nuclear spins by the static field. Pulses from radio frequency coils, in resonance with the Larmor frequency of the nuclei, cause an oscillating transverse magnetization. Measurement of the decaying response after completion of the pulse sequence leads to the NMR spectrum after processing. \nto address complex problems that are inherently quan-tum mechanical in nature. In this section, we will ex-plore three significant use cases: Nuclear Magnetic Res-onance (NMR), multireference quantum chemistry, and the study of radicals. Each of these use cases will be ex-amined through the lens of the four critical ITBQ crite-ria for evaluating quantum applications: identifying the industry problem, transforming the problem into a quan-tum format, assessing whether classical methods can suf-fice, and demonstrating the potential for quantum ad-vantage. \nA substantial effort at HQS has been dedicated to the transform to quantum step for these use cases, enabling us to extract the most relevant quantum mechanical de-grees of freedom. By systematically analyzing these use cases, we aim to highlight the unique challenges and opportunities presented by quantum computing in the realm of quantum chemistry, ultimately shedding light on the potential for achieving significant advancements in this field. But also show a more fundamental approach about how to judge use cases and guide more projects towards quantum advantage. \nA. Nuclear Magnetic Resonance \n1. Identify industry problem \nNuclear magnetic resonance (NMR) spectroscopy is one of the most important analytical techniques in chem-istry. When determining or elucidating the structure of organic compounds, it is usually NMR spectroscopy that provides the majority of information, complemented by other techniques such as infrared spectroscopy or mass spectrometry. The information content of NMR spectra is only superseded by X-ray crystallography. However, NMR spectra are usually recorded for substances in so-lution without needing to prepare crystals, rendering it much more suitable as a routine technique. In contrast to crystallographic structure determination, which often re-quires highly specialized training, interpretation of NMR spectra is considered a core skill and is introduced early in undergraduate chemistry cources [29]. Whenever or-ganic (and many inorganic) substances are synthesized in a modern lab, it is likely that NMR spectra are recorded, too. \nIn an NMR spectrometer (see Fig. 4), the sample is placed in a strong magnetic field (9.4T in a routine 400 MHz spectrometer), causing a net magnetization of the nuclear spins by the static field. Pulses from radio fre-quency coils, in resonance with the Larmor frequency of the nuclei, cause an oscillating transverse magnetization; measurement of the decaying response after completion of the pulse sequence leads to the NMR spectrum af-ter processing. Thus, NMR problems can be modelled through the time evolution of a system of nuclear spins, interacting with the external field, the oscillating pulses and among each other [30]. \nThe interaction with the molecule hosting the nuclei is mainly absorbed into two types of parameters: \n1. Nuclei in a molecule are diamagnetically shielded, leading to an energy splitting that is slightly differ-ent than that of a bare nucleus in the same mag-netic field. [31] Moreover, shielding experienced by nuclei differs within the same molecule, being influ-enced by functional groups, adjacent substituents and their relative arrangement in space. This effect is quantified through chemical shift values. Indeed, it is the chemical shifts that permit NMR to be used as an analytical tool for structure determination in the first place. \n2. Observable interactions between spins are due to indirect dipole-dipole couplings (J-couplings) be-tween spins, which are mediated by chemical bonds. These couplings are specific for the type and num-ber of bonds separating nuclei, thus providing valu-able structural information. \nA particularly common variant of NMR spectroscopy \nis 1H-NMR. Protons in molecules form systems of inter-acting spins with different chemical shifts, thus leading to a particularly complicated appearance of spectra. In practice, 1H spectra are usually assigned by identifying chemical shifts according to their characteristic values, and by analyzing coupling patterns, where possible. \nAlternatively, analysis of an NMR experiment can pro-ceed via its quantum mechanical simulation. Recent opinions in chemical and pharmaceutical publications ar-gue in favor of putting more emphasis on the rigorous simulation approach [32-34]. The size of the exact rep-resentation of the spin system scales exponentially with the number of spins. In order to set up the problem de-scription, values of the NMR parameters-chemical shifts and J-couplings are needed. These can either be fitted to the spectrum, typically in an iterative procedure re-quiring repeated spectrum simulation. Alternatively, the NMR parameters can be predicted, likely with some de-gree of deviation from experiment. Predicted values can also be used as a starting point for further refinement, or be combined with experimental parameters to substitute for missing values. \nOne important NMR application is structure deter-mination (see Fig. 5 (upper panel)), which is highly rel-evant both in academic and in industrial research: the structure of a molecule is unknown or uncertain. To elucidate the structure, it is necessary to assign NMR parameters such that they reproduce the experimental spectrum; at the same time, the values of the NMR pa-rameters need to be entirely consistent with, and not in contradiction to, the features of the proposed molecular structure. Moreover, there should be no other plausible structures that could also be in agreement with the exper-imental spectrum. To aid with structure determination, spectrum simulation can be coupled with computational NMR parameter prediction for proposed chemical struc-tures. \nAnother important NMR application is analysis of substance mixtures (see Fig. 5 (lower panel)), for ex-ample through quantitative NMR (qNMR). Important application areas include quality control, such as the pu-rity determination of pharmaceuticals; analysis of bioflu-ids, for example in metabolomics; or real-time monitor-ing of chemical or biological processes [35, 36]. In these cases, structures of target molecules are known in princi-ple, and often also their reference spectra. The goal is to determine substance concentrations, or the presence of impurities. Concentration determination through NMR is based on the principle that integrals over peaks in the spectrum are proportional to the number of resonating nuclei. In these applications, NMR often serves as an al-ternative or as a complement to mass spectrometry cou-pled with liquid or gas chromatography (e.g., GC/MS, LC/MS). \nApplying qNMR requires either spectra of reference compounds to be recorded, or libraries of reference spec-tra to be built, such as in the Human Metabolome Database [37]. Spectra of compounds depend on the \nstrength of the magnetic field specific to the spectrome-ter, but they can be recomputed if the NMR parameters are available. The NMR parameters themselves, espe-cially the chemical shifts, can be influenced by conditions such as solvent, temperature, concentration or pH value. For metabolomics, the GISSMO data base contains sets of spin parameters that have been fitted to experimental spectra for hundreds of compounds [38, 39]. \nFor the most NMR applications, typical NMR spec-trometers employ superconducting magnets to maintain high magnetic fields. Benchtop devices with lower fields generated by permanent magnets are becoming increas-ingly popular as a less expensive and more compact alter-native [40]. With field strengths of ca. 1 T to 2T, the ap-pearance of NMR spectra becomes profoundly more com-plicated, rendering accurate simulation even more useful for the analysis of such spectra. \nOur assessment of NMR simulation as an actual in-dustry problem is four out of five stars (★★★★★). The importance of nuclear magnetic resonance as an analyt-ical technique is beyond doubt. Realistic spectra can be calculated, subject to having or computing suitable NMR parameters. This use case does not receive the highest rating, since additional efforts are required to achieve a seamless integration of simulations within the established experimental analytical workflow. \n2. Transform to quantum \nA central part in the description of the NMR problem is the Spin Hamiltonian [30]: \nH = \nΣγκ (1 + δκ) ΒÎκ + 2πΣ JκικÎι. (1) \nk \nk>l \nIn this equation, B is the static magnetic field; the operator Îk = ħ⁻¹ŝk represents the spin of nucleus k, and yk is the gyromagnetic ratio of the respective iso-tope in vacuum. Chemical shifts de quantify the dia-magnetic shielding of each nucleus within the molecule. Values of chemical shifts are specified in parts per mil-lion (ppm), as the diamagnetic shielding is very weak: for example, chemical shifts of ¹H range within 30 ppm (the majority in a range of 12 ppm), those of 13C within 250 ppm. Indirect dipole-dipole coupling constants Jkl quantify the interaction between nuclei mediated through chemical bonds. These couplings are specific for the type and num-ber of bonds separating nuclei, thus providing valu-able structural information. \nThe Spin Hamiltonian presented above is sufficient for spin- nuclei of diamagnetic molecules in liquid state. Di-rect dipole interactions between nuclei cancel out due to molecular rotations occurring on a faster timescale than the NMR experiment, though they become important for macromolecules or solids. \n3. Get the job done without the quantum computer \nQuantum computing approaches to compute singlet-triplet gaps in diradicals can be benchmarked against either conventional general-purpose electronic structure methods or a custom RPA approximation designed at HQS Quantum Simulations for computing this particular property [122]. \nConventional methods include highly accurate and expensive methods based on the CASSCF reference (multireference methods): perturbation theory variants CASPT2 and NEVPT2, described above, as well as other multireference variants of coupled-cluster (MRCC) [123] and configuration interaction (MRCI) [124] theo-ries. A cheaper and way less robust alternative are spin-symmetry-broken (unrestricted) single-configurational methods (density functional theory [113]). \nAn RPA model for singlet-triplet gaps in diradicals developed at HQS [122] is based on the two-electrons-in-two orbitals model, which captures the most essential strong correlation in a diradical. The effect of empty and doubly occupied orbitals (dynamic correlation) is taken into account by RPA. In the static limit, direct RPA ap-proximation leads to the renormalization of the parame-ters of the two-orbital model. Electron interactions (one-and two-electron integrals) are taken from the minimum CASSCF-(2,2) conventional quantum chemistry calcu-lation. The computational price of such simulation is marginally larger than that of a single-configurational one, whereas the RPA correction has an exact analytic solution, adding practically no computational overhead. \nFigure 10 shows accuracy of our RPA approach as com-pared to the multireference methods as well as density functional theory (with B3LYP functional) in the unre-stricted (spin-symmetry-broken) variant. RPA performs on average better than density function theory methods and is commensurate with CASSCF-(2,2) based on the \nsame selected active space. However, CASSCF, and also NEVPT2, calculations with larger active spaces exhibit consistently better accuracy, at a price of larger compu-tational costs. \nWe rate this use case as two stars (★★★★★) since there is a wide selection of general conventional electronic structure methods varying in computational cost, accu-racy and robustness, as well as an efficient custom HQS developed RPA approach, demonstrating reasonable re-sults at moderate costs. There are still many steps that can be improved for classical calculations to achieve a better rating over time. \n4. Show quantum advantage \nWith Hamiltonian (14) we have constructed a mini-mal active space consisting of 2 electrons in 2 orbitals, resulting in four possible electronic states. This allows the electronic structure of the active space to be fully described by a 4 × 4 Hamiltonian matrix. The remaining orbitals, which are not part of the active space, have been transformed into bosonic modes using the random phase approximation, effectively modeling them as a bosonic bath. Consequently, the system can be described as a 4x4 electronic system coupled to a bosonic bath, analo-gous to the spin-boson model but with a four-level system instead of a two-level (spin) system. \nThe spin-boson model, where a two-level system is coupled to a bosonic bath, has been extensively stud-ied in the context of quantum computing and quantum simulation. Numerous approaches have been developed to solve its dynamics, including digital quantum simu-lation on platforms like IBM quantum computers, varia-tional quantum algorithms, and analog simulations using trapped ions or superconducting circuits. These methods address challenges such as modeling dissipative dynam-ics, handling strong system-bath coupling, and simulat-ing structured spectral densities [125, 126]. \nGiven the structural similarity between the spin-boson model and the 4x4 system coupled to a bosonic bath we have constructed, these established techniques should be adaptable to our situation. The primary difference lies in the dimensionality of the system (4x4 versus 2x2), which increases the complexity of the Hamiltonian but does not \nfundamentally alter the nature of the system-bath inter-action. Quantum algorithms, such as Trotterization or variational methods, can be extended to handle the larger Hilbert space of the 4x4 system, while analog platforms could be configured to represent four-level systems cou-pled to bosonic modes. \nOf course there are also differences to the existing work on the spin boson model. In the existing work, often well known bosonic spectral functions are being simu-lated. The spectral function for a molecule obviously looks quite different from an ohmic bath. So there are still many points to analyze. However, the key point of many of these efforts is that it is possible to map spin-boson model to a quantum processor with quite low depth, and this should also be the case in our situation. \nWhile this offers interesting opportunities, there is still a lot of work to be done to solve for the singlet triplet splitting within the random phase approximation on a quantum computer. Nonetheless, as compare to the di-rect approach using active space methods, we directly in-tegrate dynamic correlations in this approach, generate an extremly effective mapping to the quantum computer and are looking for energy differences in the spectrum of a Hamiltonian, instead of ground state properties. There-fore we rank the current probability to achieve quantum advantage for this use case at ★★★",
    "answer": "Nuclear Magnetic Resonance (NMR) spectroscopy is a critical analytical technique for elucidating chemical structures, making it a promising quantum computing use case due to its inherent quantum mechanical nature. NMR problems are modeled through the time evolution of a nuclear spin system interacting with external magnetic fields and internal molecular couplings.\n\nKey parameters in the Spin Hamiltonian for NMR are:\n*   **Chemical shifts (\\(\\delta_k\\)):** Quantify the diamagnetic shielding of each nucleus, leading to energy splitting variations. Values are typically in parts per million (ppm), e.g., \\(\\text{30 ppm for }^1\\text{H}\\) and \\(\\text{250 ppm for }^13\\text{C}\\).\n*   **J-couplings (\\(J_{kl}\\)):** Quantify indirect dipole-dipole interactions between spins, mediated by chemical bonds. Values are typically up to 20 Hz, but can be several hundred Hz for spins separated by a single bond.\n\n**Classical Approaches:** Exact numerical simulation of NMR spin systems scales exponentially with the number of spins (\\(O(2^N)\\)), making it infeasible for large systems (e.g., \\(>25\\) spins). Approximate solutions, such as spin clustering, reduce computational complexity by grouping strongly interacting spins. This provides accurate results for systems with relatively weak spin correlations, especially at high magnetic fields (e.g., a 50-spin molecule like Friedelin). However, for strong correlations (e.g., a 22-spin molecule like 1,2-di-tert-butyl-diphosphane), spin clustering may fail to capture relevant spin interactions, necessitating exact solutions that exploit local symmetries (like SU(2)).\n\n**Quantum Approaches:** Quantum computers can simulate NMR dynamics efficiently by evolving the system under unitary time evolution, avoiding the exponential memory cost of classical simulations. The Spin Hamiltonian is typically mapped to a minimal active space (e.g., 2 electrons in 2 orbitals) whose electronic structure can be fully described by a Hamiltonian matrix (e.g., 4x4). Remaining orbitals are transformed into bosonic modes, creating a system analogous to a spin-boson model. This structural similarity suggests that established quantum simulation techniques (e.g., on IBM quantum computers) can be adapted. Quantum simulation for NMR is promising as it involves time evolution and solving the Schrödinger equation, tasks naturally suited for quantum computers."
  },
  {
    "topic": "Quantum Error Correction > Superconducting Qubits > Dual-Rail Encoding",
    "question": "What is dual-rail encoding, and how is it used in superconducting qubits, specifically in multimode transmons, to implement erasure error conversion for improved logical qubit performance?",
    "context": "Optimal Compilation Strategies for QFT Circuits in Neutral-Atom Quantum Computing \nDingchao Gao¹, Yongming Li², Shenggang Ying¹, and Sanjiang Li³,* \nKey Laboratory of System Software (Chinese Academy of Sciences) and State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences \n2School of Mathematics and Statistics, Shaanxi Normal University, Xi'an, 710062, China \n3Centre for Quantum Software and Information (QSI), Faculty of Engineering and Information Technology, University of Technology Sydney, NSW 2007, Australia \n*sanjiang.li@uts.edu.au \nABSTRACT \nNeutral-atom quantum computing (NAQC) offers distinct advantages such as dynamic qubit reconfigurability, long coherence times, and high gate fidelities, making it a promising platform for scalable quantum computing. Despite these strengths, efficiently implementing quantum circuits like the Quantum Fourier Transform (QFT) remains a significant challenge due to atom movement overheads and connectivity constraints. This paper introduces optimal compilation strategies tailored to QFT circuits and NAQC systems, addressing these challenges for both linear and grid-like architectures. By minimizing atom movements, the proposed methods achieve theoretical lower bounds in atom movements while preserving high circuit fidelity. Comparative evaluations against state-of-the-art compilers demonstrate the superior performance of the proposed methods. These methods could serve as benchmarks for evaluating the performance of NAQC compilers. \narXiv:2506.15116v1 [quant-ph] 18 Jun 2025 \n1 Introduction \nQuantum computing has emerged as a transformative technology capable of solving complex problems in cryptography¹, database search², chemical simulations³, and machine learning⁴. The rapid advancement of quantum computing research has led to the development of multiple hardware platforms, each with unique strengths and challenges. Among these, superconducting qubits have gained prominence due to their robust control schemes and compatibility with conventional microelectronics⁵. In the past several years, neutral-atom quantum computing (NAQC) has attracted significant attention for its inherent advantages, including scalability, high qubit connectivity, long coherence times, and superior gate fidelity⁶⁻⁸. \nIn superconducting quantum devices, such as IBM's heavy-hex architecture, qubits are sparsely arranged to minimize crosstalk. This sparse arrangement, however, requires additional routing steps (mainly through inserting SWAP gates) to facilitate operations between distant qubits. In contrast, NAQC devices, often arranged on a 2D grid, enable two-qubit gates between any two qubits by shuttling them closer. However, longer shuttling distances can increase noise levels. Thus, careful circuit compilation is critical on both platforms to achieve high circuit fidelity. \nThe Quantum Fourier Transform (QFT) is a foundational component in many quantum algorithms, such as Shor's factoring algorithm¹, phase estimation', quantum simulation¹⁰, amplitude amplification¹¹, and the HHL algorithm for solving systems of linear equations¹². Despite its importance, implementing QFT efficiently on current quantum hardware represents a significant challenge. The QFT's all-to-all interaction pattern requires careful consideration of hardware-specific constraints, particularly in systems with limited qubit connectivity. Due to its core role in quantum computing, QFT optimization has been extensively studied in the literature, see¹³⁻²⁰. \nIn the realm of superconducting quantum devices, Maslov¹³ proposed a linear-depth transformation for QFT circuits on the linear nearest-neighbor (LNN) architecture, where qubits lie along a single path. Modern superconducting devices often feature more complex connectivity, making it challenging to identify a single Hamiltonian path that visits all qubits. Building on this foundational work, Jin et al.¹⁷ introduced efficient mapping techniques tailored to general 2D grid and IBM heavy-hex architectures. Focused on IBM's heavy-hex architecture, Gao et al. ¹⁹ proposed specifically optimized linear-depth transformations. \nFor NAQC devices, costly SWAP gates can be entirely replaced with atom movement (cf.²¹). While several compilation algorithms have been proposed²²⁻²⁵, they fail to produce optimal transformations. Furthermore, there is a lack of benchmarking tools to evaluate how close these generated transformations are to the optimal solutions. \nIn this work, we address these challenges by introducing optimal compilation strategies tailored to QFT circuits and NAQC \nError-detected coherence metrology of a dual-rail encoded fixed-frequency multimode superconducting qubit \nJames Wills,* Mohammad Tasnimul Haque, and Brian Vlastakis+ \nOxford Quantum Circuits, Thames Valley Science Park, Shinfield, Reading, United Kingdom, RG2 9LH \n(Dated: June 19, 2025) \nAmplitude damping is a dominant source of error in high performance quantum processors. A promising approach in quantum error correction is erasure error conversion, where errors are con-verted into detectable leakage states. Dual-rail encoding has been shown as a candidate for the conversion of amplitude-damping errors; with unique sensitivities to noise and decoherence sources. Here we present a dual-rail encoding within a single fixed-frequency superconducting multimode transmon qubit. The three island, two junction device comprises two transmonlike modes with a detuning of 0.75-1 GHz, in a coaxial circuit QED architecture. We show the logical bit-flip and phase-flip error rates are more than one order of magnitude lower than the physical error rates, and demonstrate stability and repeatability of the architecture through an extended measurement of three such devices. Finally, we discuss how the error-detected subspace can be used for investi-gations into the fundamentals of noise and decoherence in fixed-frequency transmon qubits. \nI. INTRODUCTION \nAmplitude damping as a result of energy relaxation is a significant source of error in superconducting qubits for use in quantum computing, characterized by the de-cay constant conventionally known as T₁. Significant research efforts exist to try to increase qubit T₁ via nanofabrication methods [1, 2], novel qubit designs such as fluxonium [3-5], and through engineering of the mi-crowave environment and Purcell filtering [6-10]. \nQuantum error correction exists as a method of reduc-ing errors in physical qubits via the redundant encod-ing of information across a large array of noisy physical qubits [11-14]. Whilst early demonstrations of surface codes have shown modest improvement in logical errors over physical error rates [15, 16], the hardware overheads required to sufficiently reduced errors to rates essential for useful quantum computation are dauntingly large. \nNovel encoding and error-detection mechanisms have been suggested to increase efficiency in error-correction protocols and reduce these hardware requirements. No-tably, erasure error-conversion [17, 18], defined by con-verting noise into heralded leakage states, has been the-oretically shown to have more favorable error thresholds [19-22]. As such, it may be possible to reach lower logical qubit error rates for the same number of noisy physical qubits. \nOne method of implementing an erasure error-conversion is dual-rail encoding, where quantum infor-mation is encoded across the subspace of two physical entities. This encoding has been demonstrated in su-perconducting quantum devices in both superconducting cavities [23-26], as well as flux tunable transmon qubits [27]. In each case, the encoding and error-detection pro-tocols show an improvement in the error-rates of the \nlogically encoded qubit compared to the physical con-stituents, demonstrating the viability of this architecture. \nIn this paper, we present an implementation of a dual-rail encoding in a fixed-frequency multimode transmon, within a coaxial circuit QED architecture. We demon-strate all-microwave control and readout of the logical qubit, utilizing an end-of-line error-detection protocol, and characterize idling error-rates of the encoded sub-space. The error-detected logical coherence metrics are shown to have an order of magnitude improvement in bit-flip and phase-flip error rates as compared to the con-stituent physical qubit modes. In addition, we show how the logical subspace can be used as a tool to probe the fundamentals of the noise and decoherence mechanisms in superconducting circuits. It is vital to understand these sources of error more deeply in order to achieve the goal of a fault tolerant quantum computer.",
    "answer": "`Dual-rail encoding` is a method of quantum error correction that encodes quantum information across the subspace of two physical entities, enabling `erasure error conversion`. In this technique, errors, particularly amplitude damping, are converted into detectable `leakage states`, allowing for their identification and post-selection, which can lead to lower logical qubit error rates.\n\nIn superconducting qubits, this encoding has been implemented in various devices, including multimode transmons. A multimode transmon, which consists of three superconducting islands and two Josephson junctions, provides an additional degree of freedom, enabling a second transmon-like mode. This allows for encoding a logical qubit in its single-excitation subspace, for example, mapping logical \\(|0_L\\rangle\\) to the \\(|10\\rangle\\) state and logical \\(|1_L\\rangle\\) to the \\(|01\\rangle\\) state of the two modes.\n\nKey to its error detection is the design principle that prevents direct single-photon decay between the \\(|01\\rangle\\) and \\(|10\\rangle\\) states due to different mode symmetries. Consequently, if amplitude damping occurs (e.g., either \\(|10\\rangle\\) or \\(|01\\rangle\\) decaying), the system transitions to the \\(|00\\rangle\\) state, which is outside the computational subspace. This \\(|00\\rangle\\) state acts as a detectable leakage state, effectively flagging that an error has occurred. This approach offers a fixed-frequency, hardware-efficient method for erasure error detection, simplifying calibration by negating the need for flux tuning or galvanic coupling. The result is an improvement in the error rates of the logically encoded qubit compared to the physical constituents, with logical bit-flip and phase-flip error rates observed to be more than one order of magnitude lower than their physical counterparts."
  },
  {
    "topic": "Quantum Error Correction > Superconducting Qubits > Dual-Rail Encoded Dimon Qubit (DDQ)",
    "question": "Detail the architecture and operational principles of the Dual-Rail Encoded Dimon Qubit (DDQ) in superconducting quantum computing, explaining its Hamiltonian, logical qubit encoding, and how it detects errors by converting amplitude damping into detectable leakage states.",
    "context": "II. DUAL-RAIL ENCODING IN A FIXED \nFREQUENCY MULTI-MODE TRANSMON \nQUBIT \nWhere a conventional transmon qubit consists of two superconducting islands and one Josephson junction [28], the multimode transmon consists of three superconduct-ing islands, and two Josephson junctions [29]. This ad-ditional island and junction adds a degree of freedom to the circuit, and subsequently a second transmonlike mode. These devices have been widely explored for a number of use cases such as photon shot noise suppres-sion and tunable coupling [30-32], mode-selective cou-pling and entanglement generation [33-36], and Purcell protection and fast readout [37]. \nWe construct this device in a coaxial architecture [38-40], comprising of the multimode device, and lumped ele-ment resonator fabricated on opposite sides of a low loss dielectric substrate, as shown in Fig. 1 (a). Control and readout signals are delivered by capacitively coupled coaxial lines above and below the device. In this imple-mentation, there is no galvanic coupling to the device or substrate, nor is there any requirement for flux tuning. In addition, the device, control, and readout circuitry occu-pies the same physical footprint of the conventional coax-ial qubit [29, 38], demonstrating the hardware-efficiency of this extensible architecture. \nThe device consists of two transmonlike modes we label as D and Q, due to the dipolelike and quadrupolelike polarisations of the electric fields that the modes best couple to respectively. We describe these modes as the physical modes of the system. \nThe Hamiltonian describing the circuit in the harmonic oscillator basis is given by, \nH \nħ \n2 \ni=D,Q \ni=D,Q \n(1) \nwhere wi is the transition frequency, aż is the anhar-monicity, and a at) is the annihilation (creation) operator for each mode i. The coupling between each mode is purely longitudinal with strength described by the cross-Kerr shift η. Since this device has two modes, it is known as a dimon [41, 42]. Each physical mode of the dimon behaves in the same way as a standard transmon, with transition frequencies and anharmonicities summarized in Table I. Each mode can be addressed via the same coaxial control line shown in Fig. 1 (a), and manipu-lated via a microwave signal. \nThe energy level diagram of the dimon is shown in Fig. 1 (a). The states are labeled by mn), corresponding to m(n) excitations in the D (Q) mode. We implement a dual-rail encoding by encoding a logical qubit in the single-excitation subspace of the energy level structure, such that a logical 0 (|0)L) corresponds to the (10) state, and a logical 1 (1)L) corresponds to the (01) state. No-tably, there is no direct single photon decay channel from the 01) state to the (10) state, due to the different sym-metries of the modes. We name this method of encoding, a dual-rail encoded dimon qubit (DDQ). \nAmplitude damping, or energy relaxation, will mani-fest as either the 10) or (01) state decaying to the (00) state, at a rate given by FD(Q) = 1/TD(Q), where TD(Q) is the energy relaxation time of the D (Q) mode. The dual-rail encoding converts this error to leakage outside of the logical subspace, described as an erasure error. Any detectable population of the |00) state will flag that an error has occurred. \nThe DDQ offers a fixed frequency approach to era-sure error detection within the same hardware-efficient physical footprint of the coaxial circuit QED architec-ture unit-cell. In addition, the need for flux tuning or galvanic coupling to the substrate that can introduce an unwanted noise source or crosstalk and additional cali-bration parameters is negated. \nâ \nAs shown in Fig. 1 (b), there is a lumped element LC resonator on the opposing side of the substrate. Due to the symmetry of the device, this resonator is transversely coupled to the Q mode of the dimon by a Hamiltonian term of the form ĤQR/ħ = gQR(+âQ)(a2+ân), where at is the annihilation (creation) operator of the readout resonator, and gQR is the transverse coupling strength. Whilst there is no direct coupling between the D mode and the resonator, it does inherit a state-dependent fre-quency shift due to the longitudinal coupling to the Q mode. In the dispersive limit (GQR « WR – WQ), the state-dependent frequency shifts XQ(D)R between the Q (D) mode and the readout resonator are given by, \nXQR ≈ QQ \nWR \nJQR \nWQ \n2 \nXDR≈7 (9QR2 ) \nWR \nWQ \n, \n(2) \nwhere WR is the frequency of the resonator, and 2XD(Q)R describes the full frequency shift of the resonator. \nWe perform an end-of-line (EOL) logical state distinc-tion, designed to maximally distinguish the three states",
    "answer": "The Dual-Rail Encoded Dimon Qubit (DDQ) utilizes a multimode transmon, which is a superconducting qubit with three islands and two Josephson junctions, providing two transmon-like modes (D and Q, categorized by their electric field polarizations). This architecture is implemented in a coaxial circuit QED setup, where control and readout signals are delivered via capacitively coupled coaxial lines, eliminating the need for flux tuning or galvanic coupling to the substrate, thus reducing noise and calibration complexity.\n\nThe Hamiltonian describing the DDQ in the harmonic oscillator basis is given by: \n\\(H = \\sum_{i=D,Q} \\hbar \\omega_i (a^{\\dagger}_i a_i + \\frac{1}{2}) + \\frac{\\alpha_i}{2} (a^{\\dagger}_i + a_i)^2 - \\eta a^{\\dagger}_D a_Q a_D a_Q\\)\nwhere \\(\\omega_i\\) is the transition frequency, \\(\\alpha_i\\) is the anharmonicity, and \\(\\eta\\) is the cross-Kerr shift describing the longitudinal coupling between the modes. This Hamiltonian reveals that the device acts as a 'dimon' with two distinct physical modes.\n\n**Logical Qubit Encoding:** A logical qubit is encoded in the single-excitation subspace: logical \\(|0_L\\rangle\\) corresponds to the \\(|10\\rangle\\) state (one excitation in D mode, zero in Q mode), and logical \\(|1_L\\rangle\\) corresponds to the \\(|01\\rangle\\) state (zero excitation in D mode, one in Q mode). A key operational principle is the absence of a direct single-photon decay channel from \\(|01\\rangle\\) to \\(|10\\rangle\\) due to the distinct symmetries of the modes.\n\n**Error Detection Mechanism:** Amplitude damping, or energy relaxation, from either \\(|10\\rangle\\) or \\(|01\\rangle\\) modes results in decay to the \\(|00\\rangle\\) state. This transition effectively converts the quantum error into leakage outside the logical subspace, functioning as an 'erasure error'. Any observable population in the \\(|00\\rangle\\) state acts as a flag, signaling that an error has occurred. This `end-of-line (EOL) logical state distinction` is designed to maximally differentiate between the logical states and the error-flagging \\(|00\\rangle\\) state, enabling error-detected coherence metrology."
  },
  {
    "topic": "Quantum Error Correction > Superconducting Qubits > DDQ Performance Metrics",
    "question": "Define and mathematically describe the methods for characterizing the logical bit-flip rate, logical phase-flip rate, and Ramsey coherence in the Dual-Rail Encoded Dimon Qubit (DDQ) system, including the specific measurements and decay behaviors observed.",
    "context": "III. ERROR-DETECTED COHERENCE \nMETRICS \nUsing our logical state manipulation and readout pro-tocols, we now characterize the logical error-rates of the DDQ: the bit-flip error rate, phase-flip error rate, and Ramsey coherence. \nThe logical bit-flip rate is characterized by the rate at which the 1)L population transfers to the (0) L state, and vice-versa. We measure this by preparing the DDQ in each of the two logical states, waiting for a delay time At, and performing an EOL logical state distinction readout, as shown in the inset of Fig. 2 (d). Using the readout classifier previously described in Section II, each mea-surement shot is classified into one of the three DDQ states. The logical bit-flip probability is calculated as, \n1 \nP(bit-flip) = (P(|1)z|init.|0)₁) + P(|0)z|init.|1))) \n2 \n(4) \nFollowing the methodology of [27], we measure P|1)L after initializing the DDQ in the 01) and 10) states, and plot the difference between the two error-detected \nstate populations as a function of measurement delay At, shown in Fig. 2 (d). \nWhere a conventional measurement of a superconduct-ing transmon qubit energy relaxation rate (or T₁ decay) typically shows an exponential decay, shown in Fig. 2 (a), the error-detected logical bit-flip measurement shows a more complex second-order type decay profile. At long time scales, in the absence of mid-circuit erasure error-checks, the logical bit-flip error is dominated by second order relaxation and subsequent re-excitation events. For short timescales relevant for algorithmic implementations and error-correction cycles, the decay profile can be ap-proximated to be linear. To obtain the short circuit depth bit-flip rate, we fit the decay up to 30 µs to a linear slope with constant offset to extract the logical bit-flip rate Γł, and define the error-detected logical qubit Tł = 1/Γ{. \nTo measure the logical phase-flip rate, we implement a Hahn-echo sequence. We prepare the dimon in the super-position state ) = (|01) + |10〉)/√2, wait a delay time At/2, apply a refocusing pulse, and project the state back to the logical Z axis after a second delay period of At/2, as shown in the pulse sequence in the inset of Fig. 2 (e). Once again, we perform a logical state distinction EOL measurement, and show the DDQ states as a function of delay time, At, in Fig. 2 (b). We show the postselected state probability P(|1)₁), along with the erasure fraction in Fig. 2 (e). \nAs with the bit-flip measurement, we observe a non-exponential decay in the error-detected measurement. The phase-flip error-rate for a circuit with short depth can be obtained by fitting the decay up to 30 µs with a linear decay. From this, we extract a logical phase-flip rate = 1/TE. We discuss the effect of noise sources on coherence further in Section IV. \nFinally, we measure the Ramsey coherence by prepar-ing the DDQ in a superposition state |/) = (101) + |10))/√2, allowing the state to evolve for time At, and projecting back onto the measurement axis, as shown in the pulse sequence in Fig. 2 (f). We add a virtual de-tuning of Af ≈ 75 kHz by modifying the phase, 6, of the final projection gate such that $ = 2π∆ft, inducing an oscillation between the (01) and ❘10) states. As previ-ously, we show the measured physical state populations in Fig. 2 (c), and postselected logical state probability P(10)L), as well as erasure fraction, in Fig. 2 (f). \nUnlike the bit-flip and phase-flip measurements, the Ramsey decay follows an exponentially decaying oscilla-tion of the form, \nP(|0)L) = A exp(-∆t/T/R) cos(2π∆f∆t+φο) +C, (5) \nwhere TR is the extracted Ramsey decay constant, Af is frequency detuning, and A, C and do are amplitude and phase offsets.",
    "answer": "In the Dual-Rail Encoded Dimon Qubit (DDQ) system, logical error rates are characterized by measuring bit-flip rate, phase-flip rate, and Ramsey coherence:\n\n1.  **Logical Bit-Flip Rate:** This rate quantifies the probability of a logical \\(|1_L\\rangle\\) state transferring to a logical \\(|0_L\\rangle\\) state, and vice versa. It is measured by preparing the DDQ in either logical state, waiting a delay time \\(\\Delta t\\), and then performing an End-of-Line (EOL) logical state distinction readout. The logical bit-flip probability is calculated as: \\(P_{\\text{bit-flip}} = \\frac{1}{2}(P(|1_L\\rangle|\\text{init. }|0_L\\rangle) + P(|0_L\\rangle|\\text{init. }|1_L\\rangle))\\). The observed decay profile is non-exponential, exhibiting a more complex second-order decay at long timescales due to relaxation and re-excitation. For short timescales relevant to algorithmic implementations, the logical bit-flip rate \\(\\Gamma_L^{\\downarrow}\\) (and thus \\(T_L^{\\downarrow} = 1/\\Gamma_L^{\\downarrow}\\)) is extracted by fitting the decay to a linear slope with a constant offset.\n\n2.  **Logical Phase-Flip Rate:** This rate is measured using a Hahn-echo sequence. The dimon is prepared in a superposition state \\(|\\psi\\rangle = (|01\\rangle + |10\\rangle)/\\sqrt{2}\\), allowed to evolve for \\(\\Delta t/2\\), a refocusing pulse is applied, and then the state is projected back to the logical Z-axis after another \\(\\Delta t/2\\) delay. Similar to the bit-flip measurement, the decay observed is non-exponential for the error-detected measurement. The logical phase-flip rate \\(\\Gamma_L^{\\phi}\\) (and \\(T_{2E}^L = 1/\\Gamma_L^{\\phi}\\)) is obtained by fitting the decay with a linear model for short circuit depths.\n\n3.  **Ramsey Coherence:** This is measured by preparing the DDQ in a superposition state \\(|\\psi\\rangle = (|01\\rangle + |10\\rangle)/\\sqrt{2}\\), allowing it to evolve for a time \\(\\Delta t\\), and then projecting it back onto the measurement axis. A virtual detuning (e.g., \\(\\Delta f \\approx 75\\) kHz) is applied by modifying the final projection gate's phase to induce oscillations between the \\(|01\\rangle\\) and \\(|10\\rangle\\) states. The Ramsey decay follows an exponentially decaying oscillation of the form: \\(P(|0_L\\rangle) = A \\exp(-\\Delta t/T_{2R}) \\cos(2\\pi\\Delta f \\Delta t + \\phi_0) + C\\), where \\(T_{2R}\\) is the extracted Ramsey decay constant, \\(\\Delta f\\) is the frequency detuning, and \\(A, C, \\phi_0\\) are amplitude and phase offsets."
  },
  {
    "topic": "Quantum Error Correction > Superconducting Qubits > DDQ Noise and Decoherence",
    "question": "What performance improvements were experimentally observed in the Dual-Rail Encoded Dimon Qubit (DDQ) system's coherence metrics (bit-flip, phase-flip, Ramsey), and how do different noise sources affect the DDQ's coherence compared to conventional transmons?",
    "context": "total repetitions each). Q2 and Q3 are measured simul-taneously, whilst Q1 is measured independently over a different 50 hour period. We show boxplots of the ex-tracted coherence parameters, TI, TE, and TR in Fig. 3 (Q3 Ramsey coherence was unable to be measured due to instability in one of the physical modes). \n2R \n= \nFrom these extended measurements, we extract me-dian values of T \n3.12 ms, T 2E = 0.76 ms, and \nTR = 0.07 ms, with individual DDQ device metrics shown in Table I. In Fig. 3, we also show the me-dian physical coherence parameters. The measured error-detected coherence metrics correspond to 48x improve-ments in bit-flip rates, 11x improvement in phase-flip rate, and 4x improvement in Ramsey coherence times. In addition to the qubit coherence metrics being greatly improved over a physical qubit, these metrics remain sta-ble over the course of the measurement period, as shown in Fig. 5 in Appendix B. Validating the reproducibility of the improved logical coherence metrics across multiple devices with a range of frequencies is a significant step to demonstrate the viability of the DDQ platform for future hardware integrations. \nIn addition to the error-detected coherence metrics, we measure the erasure rate, obtained from the (00) state population. This is indicated by the red trace, shown in Fig. 2 (d-f). We fit this to the form, \nAt \nP(100)) = (1 - exp(-Terature)) +D, (6) \nwhere 1/Terasure \nFerasure is the rate of leakage out \nof the computational subspace, and D is an offset due to erasure events occurring during the EOL readout. We show the extracted erasure rate, as well as physical mode relaxation rates in the time series plot in Fig. 5, and observe it qualitatively follows the same fluctuations, as expected. \nThe erasure rate is a important metric for dual-rail en-coded qubits, as it informs both the expected leakage rate during error-correction cycles, as well as sampling over-head in postselected quantum algorithm applications. \nIV. NOISE AND DECOHERENCE \nIn this implementation of dual-rail encoding, the co-herence of the logical qubit is affected by fluctuations in the detuning between the physical modes of the di-mon (δ∆ = δωρ – δωρ), in contrast to a conventional transmon qubit implementation where the coherence is sensitive to fluctuations in the transition frequency δω [28]. As such, the DDQ possesses unique sensitivities and insensitivities to physical noise sources. \nFor any such noise sources that cause a fluctuation in both wo and wp, as shown in Fig. 4 (a), the DDQ has a reduced sensitivity (or is entirely insensitive) to in terms of coherence. We describe these as common noise sources, and physical examples include charge noise [29], quasiparticle tunneling [44], and far-detuned two-levels systems (TLS) [45]. In each of these cases, the noise source fluctuates both dimon mode frequencies simulta-neously, such that A is small, or zero in perfectly sym-metrical cases. As such, if common noise sources were a dominating cause of decoherence in this device, we would expect to observe an increase in coherence of the DDQ in the logical subspace compared to the physical modes. \nWhere the coherence of the DDQ does possess a sensi-tivity is to what we describe as differential noise sources. These are noise sources that affect only one of the transi-tion frequencies of the dimon, as shown in Fig. 4 (b), such that δωρ ≠ δωρ. Examples of physical sources that can cause this kind of differential noise are frequency depen-dent control electronics noise, and nearly-resonant TLS [46-49]. \nIn the case of the DDQ, there are noise sources where the logical encoding gives a degree of protection of co-herence, but the asymmetry of the device means there is still some sensitivity. Two prominent examples of this are Josephson energy fluctuations due to critical current or dielectric noise, and resonator photon shot noise de-phasing [32]. For dephasing due to photon fluctuations in the resonator, we approximate the ratio of logical de-phasing (Γ) to average physical mode dephasing (phys) as, \nг \nΦ \nΦ \nδχ² (κ² + 4x2) \nphys² (к² + 48χ2) \n, \n(7) \nWhere X = (XQR + XDR)/2, and 8X = XQR - XDR. With the measured parameters, we estimate the logical encod-ing to result in a reduction in dephasing rate of 42%, 11% and 18%, for Q1, Q2 and Q3 respectively. \nThe Josephson junctions are also a source of asym-metry in the fixed frequency DDQ architecture, since nanofabrication methods result in a statistical spread of junction parameters. By considering fluctuations in the detuning between the physical modes of the dimon, we calculate the dephasing rate Γ4 × Δ/ΘΕJ1,2 ≈ (1 - √r)/Δ, where r = EJ1/EJ2 is the ratio of the two junction energies. In the case of perfectly symmetrical junctions (EJ1 = EJ2), the DDQ is protected from fluc-tuations in junction energies, since both junctions par-ticipate equally in each mode of the dimon. From room temperature resistance measurements, we infer a junc-tion ratio of r = 0.963, 0.959 and 0.931, for Q1, Q2 and Q3 respectively. \nIn both of these physical noise sources, the device can be further engineered with symmetry protection, through methods of symmetric qubit-resonator coupling and more favorable dx/X ratios for resonator photon shot noise dephasing protection, or Josephson junction fabri-cation postprocessing techniques [50-53] for critical cur-rent noise protection. We also postulate that increasing the coupling strength between the modes, and hence the detuning, A, reduces sensitivity to Josephson junction asymmetry, where post-fabrication tuning methods are not viable. \nTo further probe the dynamics of the Ramsey coher-ence measurements, we perform more detailed statistical analysis and methods of decoherence benchmarking [54] on the time series data of Q2. In Fig. 4 (c-f) we show the dynamics of the extracted frequency deviation from the mean oscillation frequency, ∆f, (c-d), and decay con-stants, T2R, (e-f) over the course of an extended set of repeated measurements. The logical qubit is measured independently over a separate time period, and the phys-ical mode measurements are interleaved with each other. In both the D-mode and logical qubit time series traces, we observe sudden jumps in frequency, which in the case of the logical qubit, align with drops in coherence. By contrast, we do not observe such jumps in the Q-mode traces, and the D-mode jumps do not correlate with drops in physical mode coherence. In previous studies, these jumps are attributed to TLS switching behavior, and of-ten identified by observing similar telegraph-like noise. In Fig. 4 (g-h), we show histograms of the extracted parameters T2R and Af – Af, for both the physical modes and the logical encoded qubit, from Fig. 4 (c-f). In the case of the exponential decay constant T2R, we see an improvement in the logical qubit coherence by factor",
    "answer": "The Dual-Rail Encoded Dimon Qubit (DDQ) system demonstrated significant coherence improvements over physical qubits: 48x in bit-flip rates, 11x in phase-flip rates, and 4x in Ramsey coherence times, with extracted median values of \\(T_1^L = 3.12\\) ms, \\(T_{2E}^L = 0.76\\) ms, and \\(T_{2R} = 0.07\\) ms. These metrics remained stable over extended measurement periods.\n\nDifferent noise sources affect the DDQ's coherence distinctly compared to conventional transmons:\n\n*   **Common Noise Sources:** DDQ exhibits reduced sensitivity or insensitivity to common noise sources like charge noise, quasiparticle tunneling, and far-detuned two-level systems (TLS), which cause simultaneous fluctuations in both dimon mode frequencies. This leads to an expected increase in DDQ coherence in the logical subspace compared to physical modes, as given by the logical dephasing to physical dephasing ratio \\(\\frac{\\Gamma_L}{\\Gamma_{phys}} \\approx \\frac{\\delta\\chi^2 (\\kappa^2 + 4\\chi^2)}{\\chi^2 (\\kappa^2 + 48\\chi^2)}\\).\n*   **Differential Noise Sources:** The DDQ's coherence can be sensitive to differential noise sources, which affect only one of the dimon's transition frequencies (e.g., \\(\\delta\\omega_D \\ne \\delta\\omega_Q\\)). Examples include frequency-dependent control electronics noise and nearly-resonant TLS. Josephson energy fluctuations due to critical current or dielectric noise, and resonator photon shot noise dephasing, are prominent examples where DDQ offers some protection, but retains some sensitivity due to device asymmetry.\n*   **Josephson Junction Asymmetry:** Statistical spread in Josephson junction parameters leads to asymmetry. While perfectly symmetrical junctions protect DDQ from fluctuations, real-world imperfections introduce dephasing. Increasing coupling strength between modes can reduce sensitivity to this asymmetry.\n\nDespite improvements in bit-flip and phase-flip rates, the Ramsey coherence did not show as dramatic an improvement, and the device's frequency stability remained largely unchanged. Observations of sudden frequency jumps, particularly in the logical qubit and D-mode, were attributed to TLS switching behavior, indicating persistent differential noise sensitivity."
  }
]